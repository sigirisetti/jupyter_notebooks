{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aec90b5-dc17-4356-bed5-0d16c67cf557",
   "metadata": {},
   "source": [
    "# Inspect dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be69048-29bf-4ecd-9636-63d8cb647a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspecting a Spark DataFrame\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we inspect our DataFrame more carefully.  In particular, we\n",
    "# inspect columns that represent the following types of variables:\n",
    " \n",
    "# * Primary key variable\n",
    "# * Categorical variable\n",
    "# * Continuous numerical variable\n",
    "# * Date and time variable\n",
    " \n",
    "# In the process, we introduce various Spark SQL functionality that we cover\n",
    "# more formally in subsequent modules.\n",
    "\n",
    "\n",
    "# ## Create a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"inspect\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Read the raw ride data from HDFS to a Spark SQL DataFrame\n",
    "\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# ## Inspecting a DataFrame\n",
    "\n",
    "# Print the schema:\n",
    "rides.printSchema()\n",
    "\n",
    "# Use the *pandas* library to display the DataFrame as a scrollable HTML table:\n",
    "import pandas as pd\n",
    "pd.options.display.html.table_schema=True\n",
    "rides.limit(5).toPandas()\n",
    "\n",
    "# **Note:** The `limit` method returns a DataFrame with the specified number of\n",
    "# rows; the `toPandas` method returns a pandas DataFrame.\n",
    "\n",
    "# Use the `describe` method to get basic summary statistics on each column:\n",
    "rides.describe().toPandas()\n",
    "\n",
    "# **Note:**  The `describe` method returns a DataFrame.\n",
    "\n",
    "\n",
    "# ## Inspecting a DataFrame column\n",
    "\n",
    "# ### Inspecting a primary key variable\n",
    "\n",
    "# The `id` column represents a primary key variable:\n",
    "rides.select(\"id\").show(10)\n",
    "\n",
    "# **Note:** `select` is a DataFrame method that returns a DataFrame.\n",
    "\n",
    "# The `id` column should be non-null and unique.  Count the number of missing\n",
    "# (null) values:\n",
    "rides.filter(rides.id.isNull()).count()\n",
    "\n",
    "# **Note:** `filter` is a DataFrame method that returns a DataFrame consisting\n",
    "# of the rows for which its argument is true.  `rides.id` is a Column object\n",
    "# and `isNull` is a Column method.\n",
    "\n",
    "# Count the number of rows:\n",
    "rides.count()\n",
    "\n",
    "# Count the number of distinct values:\n",
    "rides.select(\"id\").distinct().count()\n",
    "\n",
    "# Count the number of non-missing and distinct values using Column functions:\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "rides.select(count(\"*\"), count(\"id\"), countDistinct(\"id\")).show()\n",
    "\n",
    "# We have been using the DataFrame API of Spark SQL.  To use the SQL API to\n",
    "# count the number of non-missing and distinct values, first register the\n",
    "# DataFrame as a *temporary view*:\n",
    "rides.createOrReplaceTempView(\"rides_view\")\n",
    "\n",
    "# Then use the `sql` method to run a query:\n",
    "spark.sql(\"SELECT COUNT(*), COUNT(id), COUNT(DISTINCT id) FROM rides_view\").show()\n",
    "\n",
    "# **Note:** The `sql` method returns a DataFrame.\n",
    "\n",
    "\n",
    "# ### Inspecting a categorical variable\n",
    "\n",
    "# The `service` column represents a categorical variable:\n",
    "rides.select(\"service\").show(10)\n",
    "\n",
    "# **Question:** What do the missing (null) values represent?\n",
    "\n",
    "# Count the number of missing (null) values:\n",
    "rides.filter(rides.service.isNull()).count()\n",
    "\n",
    "# Count the number of distinct values:\n",
    "rides.select(\"service\").distinct().count()\n",
    "\n",
    "# Print the distinct values:\n",
    "rides.select(\"service\").distinct().show()\n",
    "\n",
    "# Count the number of rides by service:\n",
    "rides.groupby(\"service\").count().show()\n",
    "\n",
    "# Use the SQL API to count the number of rides by service:\n",
    "spark.sql(\"SELECT service, COUNT(*) FROM rides_view GROUP BY service\").show()\n",
    "  \n",
    "# Use pandas to plot the number of rides by service:\n",
    "rides.groupby(\"service\").count().toPandas().plot(x=\"service\", y=\"count\", kind=\"bar\")\n",
    "\n",
    "\n",
    "# ### Inspecting a numerical variable\n",
    "\n",
    "# The `distance` column represents a numerical variable (stored as an\n",
    "# integer):\n",
    "rides.select(\"distance\").show(10)\n",
    "\n",
    "# Use the `describe` method to compute basic summary statistics:\n",
    "rides.describe(\"distance\").show()\n",
    "\n",
    "# **Question:** Are there any missing (null) values?\n",
    "\n",
    "# Use the `approxQuantile` method to get customized quantiles:\n",
    "rides.approxQuantile(\"distance\", \\\n",
    "\tprobabilities=[0.0, 0.25, 0.5, 0.75, 1.0], \\\n",
    "\trelativeError=1e-5)\n",
    "\n",
    "# **Note:** The `approxQuantile` method returns a Python list.\n",
    "\n",
    "# **Question:** Why does Spark produce approximate quantiles?\n",
    "\n",
    "# See the documentation for more details:\n",
    "rides.approxQuantile?\n",
    "\n",
    "# Use pandas to plot a basic histogram:\n",
    "rides.select(\"distance\").toPandas().plot(kind=\"hist\")\n",
    "\n",
    "# **Warning:** `toPandas()` is dangerous in the Spark world.  Why?\n",
    " \n",
    "\n",
    "# ### Inspecting a date and time variable\n",
    "\n",
    "# The `date_time` column represents a date and time variable:\n",
    "rides.select(\"date_time\").show(10)\n",
    "\n",
    "# However, Spark read it in as a string:\n",
    "rides.select(\"date_time\").printSchema()\n",
    "\n",
    "# Use the `cast` method to convert it to a timestamp:\n",
    "dates = rides.select(\"date_time\", rides.date_time.cast(\"timestamp\").alias(\"date_time_fixed\"))\n",
    "dates.show(5)\n",
    "\n",
    "# Note that timestamps are represented by Python `datetime` objects:\n",
    "dates.head(5)\n",
    "\n",
    "# Note that the `describe` method does not generate summary statistics for date\n",
    "# and time variables (unless they are represented as strings):\n",
    "dates.describe().show(5)\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Read the raw driver data into a Spark DataFrame called `drivers`.\n",
    "\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# (2) Examine the inferred schema.  Do the data types seem appropriate?\n",
    "\n",
    "# (3) Verify the integrity of the putative primary key `id`.\n",
    "\n",
    "# (4) Inspect `birth_date`.  What data type did Spark infer?\n",
    "\n",
    "# (5) Determine the unique values of `student`.  What type of variable do you\n",
    "# think `student` is?\n",
    "\n",
    "# (6) Count the number of drivers by `vehicle_make`.  What is the most popular\n",
    "# make?\n",
    "\n",
    "# (7) Compute basic summary statistics on the `rides` column.  How does the\n",
    "# mean number of rides compare to the median?\n",
    "\n",
    "# (8) **Bonus:** Inspect additional columns of the `drivers` DataFrame.\n",
    "\n",
    "# (9) **Bonus:** Inspect the raw rider data.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrame\n",
    "# class](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.Column\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.functions\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.types\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n",
    "\n",
    "# [pandas Documentation -\n",
    "# Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
