{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6927ffec-6b05-4876-8641-9cb06a046558",
   "metadata": {},
   "source": [
    "# Running Spark on CDSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f89d28-9c35-46b8-b693-62cf83eb3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running a Spark Application from CDSW\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate how to do the following:\n",
    "# * Start a Spark application\n",
    "# * Read a comma-delimited file from HDFS into a Spark SQL DataFrame\n",
    "# * Examine the schema of a Spark SQL DataFrame\n",
    "# * Calculate the dimensions of a Spark SQL DataFrame\n",
    "# * Examine some rows of a Spark SQL DataFrame\n",
    "# * Stop a Spark application \n",
    "\n",
    "\n",
    "# ## Running a Spark Application from CDSW\n",
    "\n",
    "# * A Spark *application* runs on the Java Virtual Machine (JVM)\n",
    "\n",
    "# * An application consists of a set of Java processes\n",
    "#   * A *driver* process coordinates the work\n",
    "#   * A set of *executor* processes perform the work\n",
    "\n",
    "# * An application normally runs on the Hadoop cluster via YARN\n",
    "#   * Used to process, analyze, and model large data sets\n",
    "#   * The driver runs in the CDSW session engine\n",
    "#   * The executors run in the worker nodes\n",
    "\n",
    "# * An application can run locally within the CDSW session engine\n",
    "#   * Used to develop and test code on small data sets\n",
    "#   * The driver runs in the CDSW session engine\n",
    "#   * The executors run as threads in the context of the driver process\n",
    "\n",
    "# * Start an application by creating an instance of the `SparkSession` class\n",
    "\n",
    "# * Stop an application by calling the `stop` method of the `SparkSession`\n",
    "# instance\n",
    "\n",
    "\n",
    "# ## Starting a Spark Application\n",
    "\n",
    "# Import the `SparkSession` class from the `pyspark.sql` module:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use the `builder` attribute to construct an instance of the `SparkSession` class:\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Call the optional `master` method to specify how the application runs:\n",
    "# * Pass `yarn` to run on the Hadoop cluster via YARN\n",
    "# * Pass `local` to run in the CDSW session engine with one thread\n",
    "# * Pass `local[N]` to run in the CDSW session engine with $N$ threads\n",
    "# * Pass `local[*]` to run in the CDSW session engine with all available\n",
    "# threads\n",
    "\n",
    "# Call the optional `appName` method to specify a name for the application.\n",
    "\n",
    "# Call the required `getOrCreate` method to create the instance.\n",
    "\n",
    "# **Note:** `spark` is the conventional name for the `SparkSession` instance.\n",
    "\n",
    "# **Note:** The backslash `\\` is the Python line continuation character.\n",
    "\n",
    "# Access the `version` attribute of the `SparkSession` instance to get the\n",
    "# Spark version number:\n",
    "spark.version\n",
    "\n",
    "# **Note:**  We are using the Cloudera Distribution of Apache Spark.\n",
    "\n",
    "\n",
    "# ## Reading data into a Spark SQL DataFrame\n",
    "\n",
    "# Use the `csv` method of the `DataFrameReader` class to read the raw ride data\n",
    "# from HDFS into a DataFrame:\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "# **Note:** HDFS is the default file system for Spark in CDSW.\n",
    "\n",
    "\n",
    "# ## Examining the schema of a DataFrame\n",
    "\n",
    "# Call the `printSchema` method to print the schema:\n",
    "rides.printSchema()\n",
    "\n",
    "# Access the `columns` attribute to get a list of column names:\n",
    "rides.columns\n",
    "\n",
    "# Access the `dtypes` attribute to get a list of column names and data types:\n",
    "rides.dtypes\n",
    "\n",
    "# Access the `schema` attribute to get the schema as a instance of the `StructType` class:\n",
    "rides.schema\n",
    "\n",
    "\n",
    "# ## Computing the number of rows and columns of a DataFrame\n",
    "\n",
    "# Call the `count` method to compute the number of rows:\n",
    "rides.count()\n",
    "\n",
    "# Pass the list of column names to the Python `len` function to compute the\n",
    "# number of columns:\n",
    "len(rides.columns)\n",
    "\n",
    "\n",
    "# ## Examining a few rows of a DataFrame\n",
    "\n",
    "# Call the `show` method to print some rows of a DataFrame:\n",
    "rides.show(5)\n",
    "rides.show(5, truncate=5)\n",
    "rides.show(5, vertical=True)\n",
    "\n",
    "# Call the `head` or `take` method to get a list of `Row` objects from a\n",
    "# DataFrame:\n",
    "rides.head(5)\n",
    "rides.take(5)\n",
    "\n",
    "\n",
    "# ## Stopping a Spark Application\n",
    "\n",
    "# Call the `stop` method to stop the application:\n",
    "spark.stop()\n",
    "\n",
    "# **Note:** The Spark application will also stop when you stop the CDSW session\n",
    "# engine.\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Create a new `SparkSession` and configure Spark to run locally with one\n",
    "# thread.\n",
    "\n",
    "# (2) Read the raw driver data from HDFS.\n",
    "\n",
    "# (3) Examine the schema of the drivers DataFrame.\n",
    "\n",
    "# (4) Count the number of rows of the drivers DataFrame.\n",
    "\n",
    "# (5) Examine a few rows of the drivers DataFrame.\n",
    "\n",
    "# (6) **Bonus:** Repeat exercises (2)-(5) with the raw rider data.\n",
    "\n",
    "# (7) **Bonus:** Repeat exercises (2)-(5) with the raw ride review data.\n",
    "# **Hint:** Verify the file format before reading the data.\n",
    "\n",
    "# (8) Stop the SparkSession.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Apache Spark](https://spark.apache.org/)\n",
    "\n",
    "# [Spark Documentation](https://spark.apache.org/docs/latest/index.html)\n",
    "\n",
    "# [Spark Documentation - SQL, DataFrames, and Datasets\n",
    "# Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.SparkSession\n",
    "# class](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrame\n",
    "# class](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "# [Using CDS 2.x Powered by Apache\n",
    "# Spark](https://docs.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_dist_comp_with_Spark.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
