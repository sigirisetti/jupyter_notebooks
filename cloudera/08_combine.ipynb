{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957ac5b-14a9-4b27-815a-a6e56b350133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combining and Splitting DataFrames\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate how to combine and split DataFrames.\n",
    "\n",
    "\n",
    "# ## Combining and Splitting DataFrames\n",
    "\n",
    "# * Spark SQL supports the usual database-style joins:\n",
    "#   * Cross join\n",
    "#   * Inner join\n",
    "#   * Left semi join\n",
    "#   * Left anti join\n",
    "#   * Left outer join\n",
    "#   * Right outer join\n",
    "#   * Full outer join\n",
    "\n",
    "# * Joins are expensive in the big-data world\n",
    "#   * Perform joins early in the process\n",
    "#   * Amortize the cost over many use cases\n",
    "\n",
    "# * Spark SQL supports the following set operations:\n",
    "#   * Union\n",
    "#   * Intersection\n",
    "#   * Subtraction\n",
    "\n",
    "# * Spark SQL provides a method to split a DataFrame into random subsets\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"combine\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Joining DataFrames\n",
    "\n",
    "# We will use the following DataFrames to demonstrate joins:\n",
    "\n",
    "scientists = spark.read.csv(\"/duocar/raw/data_scientists/\", header=True, inferSchema=True)\n",
    "scientists.show()\n",
    "\n",
    "offices = spark.read.csv(\"/duocar/raw/offices/\", header=True, inferSchema=True)\n",
    "offices.show()\n",
    "\n",
    "# ### Cross join\n",
    "\n",
    "# Use the `crossJoin` DataFrame method to join every row in the left\n",
    "# (`scientists`) DataFrame with every row in the right (`offices`) DataFrame:\n",
    "scientists.crossJoin(offices).show()\n",
    "\n",
    "# **Warning:** This can result in very big DataFrames!\n",
    "\n",
    "# **Note:** Columns with the same name are not renamed.\n",
    "\n",
    "# **Note:** This is called the *Cartesian product* of the two DataFrames.\n",
    "\n",
    "# ### Inner join\n",
    "\n",
    "# Use the `join` DataFrame method with different values of the `how` argument\n",
    "# to perform other types of joins.\n",
    "\n",
    "# Use a join expression and the value `inner` to return only those rows for\n",
    "# which the join expression is true:\n",
    "scientists.join(offices, scientists.office_id == offices.office_id, \"inner\").show()\n",
    "\n",
    "# This gives us a list of data scientists associated with an office and the\n",
    "# corresponding office information.\n",
    "\n",
    "# Since the join key has the same name on both DataFrames, we can simplify the\n",
    "# join as follows:\n",
    "scientists.join(offices, \"office_id\", \"inner\").show()\n",
    "\n",
    "# Since an inner join is the default, we can further simplify the join as\n",
    "# follows:\n",
    "scientists.join(offices, \"office_id\").show()\n",
    "\n",
    "# ### Left semi join\n",
    "\n",
    "# Use the value `left_semi` to return the rows in the left DataFrame that match\n",
    "# rows in the right DataFrame:\n",
    "scientists \\\n",
    "  .join(offices, scientists.office_id == offices.office_id, \"left_semi\") \\\n",
    "  .show()\n",
    "\n",
    "# This gives us a list of data scientists associated with an office.\n",
    "\n",
    "# ### Left anti join\n",
    "\n",
    "# Use the value `left_anti` to return the rows in the left DataFrame that do\n",
    "# not match rows in the right DataFrame:\n",
    "scientists \\\n",
    "  .join(offices, scientists.office_id == offices.office_id, \"left_anti\") \\\n",
    "  .show()\n",
    "\n",
    "# This gives us a list of data scientists not associated with an office.\n",
    "\n",
    "# **Note:** You can think of the left semi and left anti joins as special types\n",
    "# of filters.\n",
    "\n",
    "# ### Left outer join\n",
    "\n",
    "# Use the value `left` or `left_outer` to return every row in the left\n",
    "# DataFrame with or without matching rows in the right DataFrame:\n",
    "scientists \\\n",
    "  .join(offices, scientists.office_id == offices.office_id, \"left_outer\") \\\n",
    "  .show()\n",
    "\n",
    "# This gives us a list of data scientists with or without an office.\n",
    "\n",
    "# ### Right outer join\n",
    "\n",
    "# Use the value `right` or `right_outer` to return every row in the right\n",
    "# DataFrame with or without matching rows in the left DataFrame:\n",
    "scientists \\\n",
    "  .join(offices, scientists.office_id == offices.office_id, \"right_outer\") \\\n",
    "  .show()\n",
    "\n",
    "# This gives us a list of offices with or without a data scientist.\n",
    "\n",
    "# **Note:** The Paris office has two data scientists.\n",
    "\n",
    "# ### Full outer join\n",
    "\n",
    "# Use the value `full`, `outer`, or `full_outer` to return the union of the\n",
    "# left outer and right outer joins (with duplicates removed):\n",
    "scientists \\\n",
    "  .join(offices, scientists.office_id == offices.office_id, \"full_outer\") \\\n",
    "  .show()\n",
    "\n",
    "# This gives us a list of all data scientists whether or not they have an\n",
    "# office and all offices whether or not they have any data scientists.\n",
    "\n",
    "# ### Example: Joining the DuoCar data\n",
    "\n",
    "# Let us join the driver, rider, and review data with the ride data.\n",
    "\n",
    "# Read the clean data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/clean/rides/\")\n",
    "drivers = spark.read.parquet(\"/duocar/clean/drivers/\")\n",
    "riders = spark.read.parquet(\"/duocar/clean/riders/\")\n",
    "reviews = spark.read.parquet(\"/duocar/clean/ride_reviews/\")\n",
    "\n",
    "# Since we want all the ride data, we will use a sequence of left outer joins:\n",
    "joined = rides \\\n",
    "  .join(drivers, rides.driver_id == drivers.id, \"left_outer\") \\\n",
    "  .join(riders, rides.rider_id == riders.id, \"left_outer\") \\\n",
    "  .join(reviews, rides.id == reviews.ride_id, \"left_outer\")\n",
    "joined.printSchema()\n",
    "\n",
    "# **Note:** We probably want to rename some columns before joining the data and\n",
    "# remove the duplicate ID columns after joining the data to make this DataFrame\n",
    "# more usable.  For example, see the `joined` data in the DuoCar data\n",
    "# repository:\n",
    "spark.read.parquet(\"/duocar/joined/\").printSchema()\n",
    "\n",
    "\n",
    "# ## Applying set operations to DataFrames\n",
    "\n",
    "# Spark SQL provides the following DataFrame methods that implement set\n",
    "# operations:\n",
    "# * `union`\n",
    "# * `intersect`\n",
    "# * `subtract`\n",
    "\n",
    "# Use the `union` method to get the union of rows in two DataFrames with\n",
    "# similar schema:\n",
    "driver_names = drivers.select(\"first_name\")\n",
    "driver_names.count()\n",
    "\n",
    "rider_names = riders.select(\"first_name\")\n",
    "rider_names.count()\n",
    "\n",
    "names_union = driver_names.union(rider_names).orderBy(\"first_name\")\n",
    "names_union.count()\n",
    "names_union.show()\n",
    "\n",
    "# Note that `union` does not remove duplicates.  Use the `distinct` method to\n",
    "# remove duplicates:\n",
    "names_distinct = names_union.distinct()\n",
    "names_distinct.count()\n",
    "names_distinct.show()\n",
    "\n",
    "# Use the `intersect` method to return rows that exist in both DataFrames:\n",
    "name_intersect = driver_names.intersect(rider_names).orderBy(\"first_name\")\n",
    "name_intersect.count()\n",
    "name_intersect.show()\n",
    "\n",
    "# Use the `subtract` method to return rows in the left DataFrame that do not\n",
    "# exist in the right DataFrame:\n",
    "names_subtract = driver_names.subtract(rider_names).orderBy(\"first_name\")\n",
    "names_subtract.count()\n",
    "names_subtract.show()\n",
    "\n",
    "\n",
    "# ## Splitting a DataFrame\n",
    "\n",
    "# Use the `randomSplit` DataFrame method to split a DataFrame into random\n",
    "# subsets:\n",
    "riders.count()\n",
    "(train, validate, test) = riders.randomSplit(weights=[0.6, 0.2, 0.2])\n",
    "(train.count(), validate.count(), test.count())\n",
    "\n",
    "# Use the `seed` argument to ensure replicability:\n",
    "(train, validate, test) = riders.randomSplit([0.6, 0.2, 0.2], seed=12345)\n",
    "(train.count(), validate.count(), test.count())\n",
    "\n",
    "# If the proportions do not add up to one, then Spark will normalize the values:\n",
    "(train, validate, test) = riders.randomSplit([60.0, 20.0, 20.0], seed=12345)\n",
    "(train.count(), validate.count(), test.count())\n",
    "\n",
    "# **Note:** The weights must be doubles.\n",
    "\n",
    "# **Note:** The same seed will result in the same random split.\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Join the `rides` DataFrame with the `reviews` DataFrame.  Keep only those\n",
    "# rides that have a review.\n",
    "rides_and_reviews = rides \\\n",
    "  .join(reviews, rides.id == reviews.ride_id, \"right_outer\")\n",
    "rides_and_reviews.printSchema()\n",
    "\n",
    "# (2) How many drivers have not provided a ride?\n",
    "id_from_drivers = drivers.select(\"id\")\n",
    "\n",
    "# Get the driver IDs from `rides` DataFrame:\n",
    "id_from_rides = rides.select(\"driver_id\").withColumnRenamed(\"driver_id\", \"id\")\n",
    "\n",
    "# Find lazy drivers using a left anti join:\n",
    "lazy_drivers1 = id_from_drivers.join(id_from_rides, \"id\", \"left_anti\")\n",
    "lazy_drivers1.count()\n",
    "lazy_drivers1.orderBy(\"id\").show(5)\n",
    "\n",
    "# Find lazy drivers using a subtraction:\n",
    "lazy_drivers2 = id_from_drivers.subtract(id_from_rides)\n",
    "lazy_drivers2.count()\n",
    "lazy_drivers2.orderBy(\"id\").show(5)\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - crossJoin DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.crossJoin)\n",
    "\n",
    "# [Spark Python API - join DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.join)\n",
    "\n",
    "# [Spark Python API - union DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.union)\n",
    "\n",
    "# [Spark Python API - intersect DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.intersect)\n",
    "\n",
    "# [Spark Python API - subtract DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.subtract)\n",
    "\n",
    "# [Spark Python API - randomSplit DataFrame\n",
    "# method](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.DataFrame.randomSplit)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
