{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af055437-10f6-402f-a54d-898c486484e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting and Evaluating Regression Models\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Introduction\n",
    "\n",
    "# * A regression algorithm is a supervised learning algorithm.\n",
    "#   * The inputs are called *features*\n",
    "#   * The output is called the *label*\n",
    "\n",
    "# * A regression model provides a prediction of a continuous numerical label.\n",
    "\n",
    "# * Spark MLlib provides several regression algorithms:\n",
    "#   * Linear Regression (with Elastic Net, Lasso, and Ridge Regression)\n",
    "#   * Isotonic Regression\n",
    "#   * Decision Tree\n",
    "#   * Random Forest\n",
    "#   * Gradient-Boosted Trees\n",
    "\n",
    "# * Spark MLlib also provides regression algorithms for special types of\n",
    "# continuous numerical labels:\n",
    "#   * Generalized Regression\n",
    "#   * Survival Regression\n",
    "\n",
    "# * Spark MLlib requires the features to be assembled into a vector of doubles column.\n",
    "\n",
    "\n",
    "# ## Scenario\n",
    "\n",
    "# We will build a regression model to predict the duration of a ride from\n",
    "# the distance of the ride.  We can then use this regression model in our\n",
    "# mobile application to provide a real-time estimate of arrival time.\n",
    "# In the demonstration, we will use linear regression.  In the exercise, we will\n",
    "# use isotonic regression.\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ## Start a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"regress\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Load the data\n",
    "\n",
    "# Read the (clean) ride data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/clean/rides/\")\n",
    "rides.printSchema()\n",
    "\n",
    "\n",
    "# ## Prepare the regression data\n",
    "\n",
    "# Select the feature and the label and filter out the cancelled rides:\n",
    "regression_data = rides.select(\"distance\", \"duration\").filter(\"cancelled = 0\")\n",
    "\n",
    "\n",
    "# ## Plot the data\n",
    "\n",
    "# Plot the ride duration versus the ride distance on a random sample of rides\n",
    "# using pandas:\n",
    "regression_data \\\n",
    "  .sample(withReplacement=False, fraction=0.1, seed=12345) \\\n",
    "  .toPandas() \\\n",
    "  .plot.scatter(x=\"distance\", y=\"duration\")\n",
    "\n",
    "\n",
    "# ## Assemble the feature vector\n",
    "\n",
    "# Import the `VectorAssembler` class from the `pyspark.ml.feature` module:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an instance of the `VectorAssembler` class:\n",
    "assembler = VectorAssembler(inputCols=[\"distance\"], outputCol=\"features\")\n",
    "\n",
    "# Call the `transform` method to assemble the feature vector:\n",
    "assembled = assembler.transform(regression_data)\n",
    "\n",
    "# Examine the transformed DataFrame:\n",
    "assembled.printSchema()\n",
    "assembled.show(5)\n",
    "\n",
    "# **Note:** The `VectorAssembler` instance is an example of a Spark MLlib\n",
    "# `Transformer`.  It takes a DataFrame as input and returns a DataFrame as\n",
    "# output via the `transform` method.\n",
    "\n",
    "\n",
    "# ## Create a train and test set\n",
    "\n",
    "# Use the `randomSplit` method to create random partitions of the data:\n",
    "(train, test) = assembled.randomSplit(weights=[0.7, 0.3], seed=23451)\n",
    "\n",
    "# We will fit the regression model on the `train` DataFrame\n",
    "# and evaluate it on the `test` DataFrame.\n",
    "\n",
    "\n",
    "# ## Specify a linear regression model\n",
    "\n",
    "# Import the `LinearRegression` class from the `pyspark.ml.regression` module:\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create an instance of the `LinearRegression` class:\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"duration\")\n",
    "\n",
    "# Examine additional hyperparameters;\n",
    "print(lr.explainParams())\n",
    "\n",
    "\n",
    "# ## Fit the linear regression model\n",
    "\n",
    "# Call the `fit` method to fit the linear regression model on the `train` data:\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# The `fit` method returns an instance of the `LinearRegressionModel` class:\n",
    "type(lr_model)\n",
    "\n",
    "# **Note:** The `LinearRegression` instance is an example of a Spark MLlib\n",
    "# `Estimator`.  It takes a DataFrame as input and returns a `Transformer` as\n",
    "# output via the `fit` method.\n",
    "\n",
    "\n",
    "# ## Examine the model parameters\n",
    "\n",
    "# Access the `intercept` and `coefficients` attributes to get the intercept and\n",
    "# slope (for each feature) of the linear regression model:\n",
    "lr_model.intercept\n",
    "lr_model.coefficients\n",
    "\n",
    "# The slope is stored as a `DenseVector`.  Call the `toArray` method to convert\n",
    "# the DenseVector to a NumPy array:\n",
    "lr_model.coefficients.toArray()\n",
    "\n",
    "\n",
    "# ## Examine various model performance measures\n",
    "\n",
    "# The `summary` attribute is an instance of the `LinearRegressionTrainingSummary` class:\n",
    "type(lr_model.summary)\n",
    "\n",
    "# It contains a number of model performance measures:\n",
    "lr_model.summary.r2\n",
    "lr_model.summary.rootMeanSquaredError\n",
    "\n",
    "\n",
    "# ## Examine various model diagnostics\n",
    "\n",
    "# The `summary` attribute also contains various model diagnostics:\n",
    "lr_model.summary.coefficientStandardErrors\n",
    "lr_model.summary.tValues\n",
    "lr_model.summary.pValues\n",
    "\n",
    "# **Important:** The first element of each list corresponds to the slope and\n",
    "# the last element corresponds to the intercept.\n",
    "\n",
    "# **Note:** The summary attribute contains additional useful information.\n",
    "  \n",
    "\n",
    "# ## Apply the linear regression model to the test data\n",
    "\n",
    "# Use the `transform` method to apply the linear regression model to the `test`\n",
    "# DataFrame:\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "# The `transform` method adds a column to the DataFrame with the predicted\n",
    "# label:\n",
    "predictions.printSchema()\n",
    "predictions.show(5)\n",
    "\n",
    "\n",
    "# ## Evaluate the linear regression model on the test data\n",
    "\n",
    "# Import the `RegressionEvaluator` class from the `pyspark.ml.evaluation` module:\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an instance of the `RegressionEvaluator` class:\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"duration\", metricName=\"r2\")\n",
    "\n",
    "# Call the `explainParams` method to see other metrics:\n",
    "print(evaluator.explainParams())\n",
    "\n",
    "# Use the `evaluate` method to compute the metric on the `predictions` DataFrame:\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "# Use the `setMetricName` method to change the metric:\n",
    "evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "\n",
    "# **Note:** You can also use the `evaluate` method of the `LinearRegressionModel` class.\n",
    "\n",
    "\n",
    "# ## Plot the linear regression model\n",
    "\n",
    "def plot_lr_model():\n",
    "  pdf = predictions.sample(withReplacement=False, fraction= 0.1, seed=34512).toPandas()\n",
    "  plt.scatter(\"distance\", \"duration\", data=pdf)\n",
    "  plt.plot(\"distance\", \"prediction\", color=\"black\", data=pdf)\n",
    "  plt.xlabel(\"Distance (m)\")\n",
    "  plt.ylabel(\"Duration (s)\")\n",
    "  plt.title(\"Linear Regression Model\")\n",
    "plot_lr_model()\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# In the following exercises we use *isotonic regression* to fit a monotonic\n",
    "# function to the data.\n",
    "\n",
    "# (1)  Import the `IsotonicRegression` class from the regression module.\n",
    "\n",
    "# (2)  Create an instance of the `IsotonicRegression` class.  Use the same\n",
    "# features and label that we used for our linear regression model.\n",
    "\n",
    "# (3)  Fit the isotonic regression model on the train data.  Note that this\n",
    "# will produce an instance of the `IsotonicRegressionModel` class.\n",
    "\n",
    "# (4)  The model parameters are available in the `boundaries` and `predictions`\n",
    "# attributes of the isotonic regression model.  Print these attributes.\n",
    "\n",
    "# (5) Apply the isotonic regression model to the train data using the\n",
    "# `transform` method.\n",
    "\n",
    "# (6) Use the `RegressionEvaluator` to compute the RMSE on the train data.\n",
    "\n",
    "# (7) Repeat (5) and (6) on the test data.  Compare the RMSE for the isotonic\n",
    "# regression model to the RMSE for the linear regression model.\n",
    "\n",
    "# (8) Bonus: Plot the isotonic regression model.  In particular, plot the\n",
    "# `predictions` attribute versus the `boundaries` attribute.  You must convert\n",
    "# each attribute from a Spark `DenseVector` to a NumPy array using the\n",
    "# `toArray` method.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Documentation - Classification and regression](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.feature module](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.regression module](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.regression)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.evaluation module](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation)\n",
    "\n",
    "\n",
    "# ## Stop the SparkSession\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
