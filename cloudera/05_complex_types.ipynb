{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9528b-729e-4702-bc96-722be08b37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transforming DataFrame Columns\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# Spark SQL supports various column types and provides a variety of functions\n",
    "# and Column methods that can be applied to each type.  In this module we\n",
    "# demonstrate how to transform DataFrame columns of various types.\n",
    "\n",
    "# * Working with numerical columns\n",
    "# * Working with string columns\n",
    "# * Working with datetime columns\n",
    "# * Working with Boolean columns\n",
    "\n",
    "\n",
    "# ## Spark SQL Data Types\n",
    "\n",
    "# * Spark SQL data types are defined in the `pyspark.sql.types` module\n",
    "\n",
    "# * Spark SQL supports the following basic data types:\n",
    "#   * NullType\n",
    "#   * StringType\n",
    "#   * Byte array data type\n",
    "#     * BinaryType\n",
    "#   * BooleanType\n",
    "#   * Integer data types\n",
    "#     * ByteType\n",
    "#     * ShortType\n",
    "#     * IntegerType\n",
    "#     * LongType\n",
    "#   * Fixed-point data type\n",
    "#     * DecimalType\n",
    "#   * Floating-point data types\n",
    "#     * FloatType\n",
    "#     * DoubleType\n",
    "#   * Date and time data types\n",
    "#     * DateType\n",
    "#     * TimestampType\n",
    "\n",
    "# * Spark also supports the following complex (collection) types:\n",
    "#   * ArrayType\n",
    "#   * MapType\n",
    "#   * StructType\n",
    "\n",
    "# * Spark SQL provides various methods and functions that can be applied to the\n",
    "# various data types\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"columns\").getOrCreate()\n",
    "\n",
    "# Read the raw data from HDFS:\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", header=True, inferSchema=True)\n",
    "drivers = spark.read.csv(\"/duocar/raw/drivers/\", header=True, inferSchema=True)\n",
    "riders = spark.read.csv(\"/duocar/raw/riders/\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# ## Working with numerical columns\n",
    "\n",
    "# ### Example 1: Converting ride distance from meters to miles\n",
    "\n",
    "from pyspark.sql.functions import col, round\n",
    "rides \\\n",
    "  .select(\"distance\", round(col(\"distance\") / 1609.344, 2).alias(\"distance_in_miles\")) \\\n",
    "  .show(5)\n",
    "\n",
    "# **Notes:**\n",
    "# * We have used the fact that 1 mile = 1609.344 meters.\n",
    "# * We have used the `round` function to round the result to two decimal places.\n",
    "# * We have used the `alias` method to rename the column.\n",
    "\n",
    "# To add a new column, use the `withColumn` method with a new column name:\n",
    "rides \\\n",
    "  .withColumn(\"distance_in_miles\", round(col(\"distance\") / 1609.344, 2)) \\\n",
    "  .printSchema()\n",
    "\n",
    "# To replace an existing column, use the `withColumn` method with an existing\n",
    "# column name:\n",
    "rides \\\n",
    "  .withColumn(\"distance\", round(col(\"distance\") / 1609.344, 2)) \\\n",
    "  .printSchema()\n",
    "\n",
    "# ### Example 2: Converting the ride id from an integer to a string\n",
    "\n",
    "# Use the `format_string` function to convert `id` to a left-zero-padded string:\n",
    "from pyspark.sql.functions import format_string\n",
    "rides \\\n",
    "  .withColumn(\"id_fixed\", format_string(\"%010d\", \"id\")) \\\n",
    "  .select(\"id\", \"id_fixed\") \\\n",
    "  .show(5)\n",
    "\n",
    "# **Note:** We have used the [printf format\n",
    "# string](https://en.wikipedia.org/wiki/Printf_format_string) `%010d` to\n",
    "# achieve the desired format.\n",
    "\n",
    "# ### Example 3: Converting the student flag from an integer to a Boolean\n",
    "\n",
    "# Using a Boolean expression:\n",
    "riders \\\n",
    "  .withColumn(\"student_boolean\", col(\"student\") == 1) \\\n",
    "  .select(\"student\", \"student_boolean\") \\\n",
    "  .show(5)\n",
    "\n",
    "# Using the `cast` method:\n",
    "riders \\\n",
    "  .withColumn(\"student_boolean\", col(\"student\").cast(\"boolean\")) \\\n",
    "  .select(\"student\", \"student_boolean\") \\\n",
    "  .show(5)\n",
    "\n",
    "\n",
    "# ## Working with string columns\n",
    "\n",
    "# ### Example 4: Normalizing a string column\n",
    "\n",
    "# Use the `trim` and `upper` functions to normalize `riders.sex`:\n",
    "from pyspark.sql.functions import trim, upper\n",
    "riders \\\n",
    "  .withColumn(\"gender\", upper(trim(col(\"sex\")))) \\\n",
    "  .select(\"sex\", \"gender\") \\\n",
    "  .show(5)\n",
    "\n",
    "# ### Example 5: Extracting a substring from a string column\n",
    "\n",
    "# The [Census Block Group](https://en.wikipedia.org/wiki/Census_block_group) is\n",
    "# the first 12 digits of the [Census\n",
    "# Block](https://en.wikipedia.org/wiki/Census_block).  Use the `substring`\n",
    "# function to extract the Census Block Group from `riders.home_block`:\n",
    "from pyspark.sql.functions import substring\n",
    "riders \\\n",
    "  .withColumn(\"home_block_group\", substring(\"home_block\", 1, 12)) \\\n",
    "  .select(\"home_block\", \"home_block_group\") \\\n",
    "  .show(5)\n",
    "\n",
    "# ### Example 6: Extracting a substring using a regular expression\n",
    "\n",
    "# Use the `regexp_extract` function to extract the Census Block Group via a\n",
    "# regular expression:\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "riders \\\n",
    "  .withColumn(\"home_block_group\", regexp_extract(\"home_block\", \"^(\\d{12}).*\", 1)) \\\n",
    "  .select(\"home_block\", \"home_block_group\") \\\n",
    "  .show(5)\n",
    "\n",
    "# **Note:** The third argument to `regexp_extract` is the capture group.\n",
    "\n",
    "\n",
    "# ## Working with date and timestamp columns\n",
    "\n",
    "# ### Example 7: Converting a timestamp to a date \n",
    "\n",
    "# Note that `riders.birth_date` and `riders.start_date` were read in as timestamps:\n",
    "riders.select(\"birth_date\", \"start_date\").show(5)\n",
    "\n",
    "# Use the `cast` method to convert `riders.birth_date` to a date:\n",
    "riders \\\n",
    "  .withColumn(\"birth_date_fixed\", col(\"birth_date\").cast(\"date\")) \\\n",
    "  .select(\"birth_date\", \"birth_date_fixed\") \\\n",
    "  .show(5)\n",
    "\n",
    "# Alternatively, use the `to_date` function:\n",
    "from pyspark.sql.functions import to_date\n",
    "riders \\\n",
    "  .withColumn(\"birth_date_fixed\", to_date(\"birth_date\")) \\\n",
    "  .select(\"birth_date\", \"birth_date_fixed\") \\\n",
    "  .show(5)\n",
    "\n",
    "# ### Example 8: Converting a string to a timestamp\n",
    "\n",
    "# Note that `rides.date_time` was read in as a string:\n",
    "rides.printSchema()\n",
    "\n",
    "# Use the `cast` method to convert it to a timestamp:\n",
    "rides \\\n",
    "  .withColumn(\"date_time_fixed\", col(\"date_time\").cast(\"timestamp\")) \\\n",
    "  .select(\"date_time\", \"date_time_fixed\") \\\n",
    "  .show(5)\n",
    "\n",
    "# Alternatively, use the `to_timestamp` function:\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "rides \\\n",
    "  .withColumn(\"date_time_fixed\", to_timestamp(\"date_time\", format=\"yyyy-MM-dd HH:mm\")) \\\n",
    "  .select(\"date_time\", \"date_time_fixed\") \\\n",
    "  .show(5)\n",
    "\n",
    "# ### Example 9: Computing the age of each rider\n",
    "\n",
    "# Use the `current_date` and `months_between` functions to compute the age of\n",
    "# each rider:\n",
    "from pyspark.sql.functions import current_date, months_between, floor\n",
    "riders \\\n",
    "  .withColumn(\"today\", current_date()) \\\n",
    "  .withColumn(\"age\", floor(months_between(\"today\", \"birth_date\") / 12)) \\\n",
    "  .select(\"birth_date\", \"today\", \"age\") \\\n",
    "  .show(5)\n",
    "\n",
    "# **Note:** Spark implicitly casts `birth_date` or `today` as necessary.  It is\n",
    "# probably safer to explicitly cast one of these columns before computing the\n",
    "# number of months between.\n",
    "\n",
    "\n",
    "# ## Working with Boolean columns\n",
    "\n",
    "# ### Example 10: Predefining a Boolean column expression\n",
    "\n",
    "# You can predefine a Boolean column expression:\n",
    "studentFilter = col(\"student\") == 1\n",
    "type(studentFilter)\n",
    "\n",
    "# You can use the predefined expression to create a new column:\n",
    "riders \\\n",
    "  .withColumn(\"student_boolean\", studentFilter) \\\n",
    "  .select(\"student\", \"student_boolean\") \\\n",
    "  .show(5)\n",
    "\n",
    "# Or filter a DataFrame:\n",
    "riders \\\n",
    "  .filter(studentFilter) \\\n",
    "  .select(\"student\") \\\n",
    "  .show(5)\n",
    "\n",
    "# ### Example 11: Working with multiple Boolean column expressions\n",
    "\n",
    "# Predefine the Boolean column expressions:\n",
    "studentFilter = col(\"student\") == 1\n",
    "maleFilter = col(\"sex\") == \"male\"\n",
    "\n",
    "# Create a new column using the AND (`&`) operator:\n",
    "riders.select(\"student\", \"sex\", studentFilter & maleFilter).show(15)\n",
    "\n",
    "# Create a new column using the OR (`|`) operator:\n",
    "riders.select(\"student\", \"sex\", studentFilter | maleFilter).show(15)\n",
    "\n",
    "# **Important:** The Boolean column expression parser in Spark SQL is not very\n",
    "# advanced.  Use parentheses liberally in your expressions.\n",
    "\n",
    "# Note the difference in how nulls are treated in the computation:\n",
    "# * true & null = null\n",
    "# * false & null = false\n",
    "# * true | null = true\n",
    "# * false | null = null\n",
    "\n",
    "# ### Example 12: Using multiple Boolean expressions in a filter\n",
    "\n",
    "# Use `&` for a logical AND:\n",
    "riders.filter(maleFilter & studentFilter).select(\"student\", \"sex\").show(5)\n",
    "\n",
    "# This is equivalent to\n",
    "riders.filter(maleFilter).filter(studentFilter).select(\"student\", \"sex\").show(5)\n",
    "\n",
    "# Use `|` for a logical OR:\n",
    "riders.filter(maleFilter | studentFilter).select(\"student\", \"sex\").show(5)\n",
    "\n",
    "# Be careful with missing (null) values:\n",
    "riders.select(\"sex\").distinct().show()\n",
    "riders.filter(col(\"sex\") != \"male\").select(\"sex\").distinct().show()\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Extract the hour of day and day of week from `rides.date_time`.\n",
    "\n",
    "# (2) Convert `rides.duration` from seconds to minutes.\n",
    "\n",
    "# (3) Convert `rides.cancelled` to a Boolean column.\n",
    "\n",
    "# (4) Create an integer column named `five_star_rating` that is 1.0 if the ride\n",
    "# received a five-star rating and 0.0 otherwise.\n",
    "\n",
    "# (5) Create a new column containing the full name for each driver.\n",
    "\n",
    "# (6) Create a new column containing the average star rating for each driver.\n",
    "\n",
    "# (7) Find the rider names that are most similar to `Brian`.  **Hint:** Use the\n",
    "# [Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) function.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.types\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrame\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.Column\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.functions\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
