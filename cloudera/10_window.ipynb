{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bb880-9b4a-42b5-9299-442bf8808fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Window Functions\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate how to create and apply window functions.\n",
    "\n",
    "\n",
    "# ## Window Functions\n",
    "\n",
    "# * Spark SQL supports the following window functions:\n",
    "#   * `cume_dist`\n",
    "#   * `dense_rank`\n",
    "#   * `lag`\n",
    "#   * `lead`\n",
    "#   * `ntile`\n",
    "#   * `percent_rank`\n",
    "#   * `rank`\n",
    "#   * `row_number`\n",
    "\n",
    "# * Aggregate and window functions are applied `over` a window specification\n",
    "\n",
    "# * A window specification consists of at least one of the following:\n",
    "#   * Partitioning column\n",
    "#   * Ordering column\n",
    "#   * Row specification\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "# Read the enhanced ride data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/joined/\")\n",
    "\n",
    "\n",
    "# ## Example: Cumulative Count and Sum\n",
    "\n",
    "# Create a simple DataFrame:\n",
    "df = spark.range(10)\n",
    "df.show()\n",
    "\n",
    "# Create a simple window specification:\n",
    "from pyspark.sql.window import Window\n",
    "ws = Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "type(ws)\n",
    "\n",
    "# Use the window specification to compute cumulative count and sum:\n",
    "from pyspark.sql.functions import count, sum\n",
    "df.select(\"id\", count(\"id\").over(ws).alias(\"cum_cnt\"), sum(\"id\").over(ws).alias(\"cum_sum\")).show()\n",
    "\n",
    "# **Tip:** Examine the default column name to gain additional insight (if you\n",
    "# are SQL literate):\n",
    "df.select(\"id\", count(\"id\").over(ws), sum(\"id\").over(ws)).printSchema()\n",
    "\n",
    "\n",
    "# ## Example: Compute average days between rides for each rider\n",
    "\n",
    "# Create window specification:\n",
    "ws = Window.partitionBy(\"rider_id\").orderBy(\"date_time\")\n",
    "\n",
    "# Use the `lag` function to extract the date and time of the previous ride:\n",
    "from pyspark.sql.functions import lag\n",
    "rides2 = rides.withColumn(\"date_time_previous\", lag(\"date_time\").over(ws))\n",
    "rides2.select(\"rider_id\", \"date_time\", \"date_time_previous\").show(truncate=False)\n",
    "\n",
    "# **Note:** A rider's first ride does not have a previous ride; therefore, the\n",
    "# value is set to null.\n",
    "\n",
    "# Compute the number of days between consecutive rides:\n",
    "from pyspark.sql.functions import datediff\n",
    "rides3 = rides2.withColumn(\"days_between_rides\", datediff(\"date_time\", \"date_time_previous\"))\n",
    "rides3.select(\"rider_id\", \"date_time\", \"date_time_previous\", \"days_between_rides\").show(truncate=False)\n",
    "\n",
    "# Compute the average days between rides for each rider:\n",
    "from pyspark.sql.functions import count, mean\n",
    "rides4 = rides3 \\\n",
    "  .groupBy(\"rider_id\") \\\n",
    "  .agg(count(\"*\").alias(\"num_rides\"), mean(\"days_between_rides\").alias(\"mean_days_between_rides\"))\n",
    "\n",
    "# Compute top and bottom 10 riders:\n",
    "rides4 \\\n",
    "  .where(rides4.mean_days_between_rides.isNotNull()) \\\n",
    "  .orderBy(\"mean_days_between_rides\") \\\n",
    "  .show(10)\n",
    "\n",
    "rides4 \\\n",
    "  .orderBy(\"mean_days_between_rides\", ascending=False) \\\n",
    "  .show(10)\n",
    "\n",
    "# **Question:** How can we make this analysis better?\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) What is the average time between rides for each driver?\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.Window\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.Window)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.WindowSpec\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.WindowSpec)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
