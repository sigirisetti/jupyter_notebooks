{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd72e3c-72e8-46fe-bb61-0ae353e5f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Processing Text and Fitting and Evaluating Topic Models\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Introduction\n",
    "\n",
    "# * Topic modeling algorithms such as Latent Dirichlet allocation extract\n",
    "# themes from a set of text documents\n",
    "\n",
    "# * Clustering algorithms such as K-means and Gaussian mixture models assume\n",
    "# that an observation belongs to one and only one cluster\n",
    "\n",
    "# * Latent Dirichlet allocation assumes that each document belongs to one or\n",
    "# more topics\n",
    "\n",
    "# * The number of topics is a hyperparameter\n",
    "\n",
    "# * Topic models can be used to categorize news articles\n",
    "\n",
    "\n",
    "# ## Scenario\n",
    "\n",
    "# In this demonstration we will use latent Dirichlet allocation (LDA) to look\n",
    "# for topics in the ride reviews.  We will also perform some basic natural\n",
    "# language processing (NLP) to prepare the data for LDA.\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# None\n",
    "\n",
    "\n",
    "# ## Create a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"text\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Load the data\n",
    "\n",
    "# Read the (clean) ride review data from HDFS:\n",
    "reviews = spark.read.parquet(\"/duocar/clean/ride_reviews/\")\n",
    "reviews.head(5)\n",
    "\n",
    "\n",
    "# ## Extract and transform features\n",
    "\n",
    "# The ride reviews are not in a form amenable to machine learning algorithms.\n",
    "# Spark MLlib provides a number of feature extractors and feature transformers\n",
    "# to preprocess the ride reviews into a form appropriate for modeling.\n",
    "\n",
    "\n",
    "# ### Parse the ride reviews\n",
    "\n",
    "# Use the\n",
    "# [Tokenizer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer)\n",
    "# class to tokenize the reviews:\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(reviews)\n",
    "tokenized.drop(\"ride_id\").head(5)\n",
    "\n",
    "# Note that punctuation is not being handled properly.  Use the\n",
    "# [RegexTokenizer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer)\n",
    "# class to improve the tokenization:\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", gaps=False, pattern=\"[a-zA-Z-']+\")\n",
    "tokenized = tokenizer.transform(reviews)\n",
    "tokenized.drop(\"ride_id\").head(5)\n",
    "\n",
    "# The arguments `gaps` and `pattern` are set to extract words consisting of\n",
    "# lowercase letters, uppercase letters, hyphens, and apostrophes.\n",
    "\n",
    "# Define a function to plot a word cloud:\n",
    "def plot_word_cloud(df, col):\n",
    "  # Compute the word count:\n",
    "  from pyspark.sql.functions import explode\n",
    "  word_count = df.select(explode(col).alias(\"word\")).groupBy(\"word\").count().collect()\n",
    "  # Generate the word cloud image:\n",
    "  from wordcloud import WordCloud\n",
    "  wordcloud = WordCloud(random_state=12345).fit_words(dict(word_count))\n",
    "  # Display the word cloud image:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "\n",
    "# Plot the word cloud:\n",
    "plot_word_cloud(tokenized, \"words\")\n",
    "\n",
    "\n",
    "# ### Remove common (stop) words from each review\n",
    "\n",
    "# Note that the ride reviews contain a number of common words such as \"the\"\n",
    "# that we do not expect to be relevant.\n",
    "# Use the\n",
    "# [StopWordsRemover](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)\n",
    "# class to remove these so-called *stop words*:\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"relevant_words\")\n",
    "remover.getStopWords()[:10]\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"words\", \"relevant_words\").head(5)\n",
    "\n",
    "# Plot the word cloud:\n",
    "plot_word_cloud(removed, \"relevant_words\")\n",
    "\n",
    "\n",
    "# ### Count the frequency of words in each review\n",
    "\n",
    "# Use the\n",
    "# [CountVectorizer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer)\n",
    "# class to compute the term frequency:\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "vectorizer = CountVectorizer(inputCol=\"relevant_words\", outputCol=\"word_count_vector\", vocabSize=100)\n",
    "\n",
    "# The `fit` method computes the top $N$ words where $N$ is set via the\n",
    "# `vocabSize` hyperparameter:\n",
    "vectorizer_model = vectorizer.fit(removed)\n",
    "vectorizer_model.vocabulary[:10]\n",
    "\n",
    "# The `transform` method counts the number of times each vocabulary word\n",
    "# appears in each review:\n",
    "vectorized = vectorizer_model.transform(removed)\n",
    "vectorized.select(\"words\", \"word_count_vector\").head(5)\n",
    "\n",
    "# **Note:** The resulting word count vector is stored as a\n",
    "# [SparseVector](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.linalg.SparseVector).\n",
    "\n",
    "\n",
    "# ## Specify and fit a topic model using latent Dirichlet allocation (LDA)\n",
    "\n",
    "# Use the `LDA` class to specify an LDA model:\n",
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(featuresCol=\"word_count_vector\", k=2, seed=23456)\n",
    "\n",
    "# Use the `explainParams` method to examine additional hyperparameters:\n",
    "print(lda.explainParams())\n",
    "\n",
    "# Use the `fit` method to fit the LDA model:\n",
    "lda_model = lda.fit(vectorized)\n",
    "\n",
    "# The resulting model is an instance of the `LDAModel` class:\n",
    "type(lda_model)\n",
    "\n",
    "\n",
    "# ## Examine the LDA topic model fit\n",
    "\n",
    "lda_model.logLikelihood(vectorized)\n",
    "lda_model.logPerplexity(vectorized)\n",
    "\n",
    "\n",
    "# ## Examine the LDA topic model\n",
    "\n",
    "# Examine the estimated distribution of topics:\n",
    "lda_model.estimatedDocConcentration()\n",
    "\n",
    "# Examine the estimated distribution of words for each topic:\n",
    "lda_model.topicsMatrix()\n",
    "\n",
    "# Examine the most important words in each topic:\n",
    "lda_model.describeTopics().head(5)\n",
    "\n",
    "# Plot the terms and weights for each topic:\n",
    "def plot_topics(model, n_terms, vocabulary):\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  rows = model.describeTopics(n_terms).collect()\n",
    "  for row in rows:\n",
    "    title = \"Topic %s\" % row[\"topic\"]\n",
    "    x = row[\"termWeights\"]\n",
    "    y = [vocabulary[i] for i in row[\"termIndices\"]]\n",
    "    plt.figure()\n",
    "    sns.barplot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Weight\")\n",
    "plot_topics(lda_model, 5, vectorizer_model.vocabulary)\n",
    "\n",
    " \n",
    "# ## Apply the topic model\n",
    "\n",
    "predictions = lda_model.transform(vectorized)\n",
    "predictions.select(\"review\", \"topicDistribution\").head(5)\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Fit an LDA model with $k=3$ topics.\n",
    "\n",
    "# * Use the `setK` method to change the number of topics for the `lda` instance:\n",
    "\n",
    "# * Use the `fit` method to fit the LDA model to the `vectorized` DataFrame:\n",
    "\n",
    "# * Use the `plot_topics` function to examine the topics:\n",
    "\n",
    "# * Use the `transform` method to apply the LDA model to the `vectorized` DataFrame:\n",
    "\n",
    "# * Print out a few rows of the transformed DataFrame:\n",
    "\n",
    "# (2) Use the `NGram` transformer to generate pairs of words (bigrams) from the tokenized reviews.\n",
    "\n",
    "# * Import the `NGram` class from the `pyspark.ml.feature` module:\n",
    "\n",
    "# * Create an instance of the `NGram` class:\n",
    "\n",
    "# * Use the `transform` method to apply the `NGram` instance to the `tokenized` DataFrame:\n",
    "\n",
    "# * Print out a few rows of the transformed DataFrame:\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Wikipedia - Topic model](https://en.wikipedia.org/wiki/Topic_model)\n",
    "\n",
    "# [Wikipedia - Latent Dirichlet\n",
    "# allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "# [Spark Documentation - Latent Dirichlet\n",
    "# allocation](http://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.clustering.LDA\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.clustering.LDAModel\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDAModel)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.clustering.LocalLDAModel\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LocalLDAModel)\n",
    "\n",
    "# [Spark Python API - psypark.ml.clustering.DistributedLDAModel\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.DistributedLDAModel)\n",
    "\n",
    "\n",
    "# ## Stop the SparkSession\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
