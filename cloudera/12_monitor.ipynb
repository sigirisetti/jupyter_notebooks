{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb50e9-d6fb-440a-9b41-69fc64c7470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Monitoring, Tuning, and Configuring Spark Applications\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Monitoring Spark Applications\n",
    "\n",
    "# We monitor a *Spark application* via the *Spark UI*.  The Spark UI is not\n",
    "# available until we start a Spark application.  We start a Spark application\n",
    "# by creating a `SparkSession` instance:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"monitor\").getOrCreate()\n",
    "\n",
    "# A link to the Spark UI is available at the top of the CDSW console pane.\n",
    "\n",
    "# **Important:** If the Spark UI link brings up a blank page, then you can\n",
    "# access the Spark UI via the Spark History Server (SHS) or directly at\n",
    "#```\n",
    "#http://spark-<session>.cdsw-gateway.<cluster>.duocar.us/\n",
    "#```\n",
    "# where `session` and `cluster` are listed in the session URL\n",
    "#```\n",
    "#http://cdsw-gateway.<cluster>.duocar.us/<user>/<project>/engines/<session>\n",
    "#```\n",
    "\n",
    "\n",
    "# ### Example 1: Partitioning DataFrames\n",
    "\n",
    "# Read the ride data as a text file:\n",
    "rides = spark.read.text(\"/duocar/raw/rides/\")\n",
    "\n",
    "# View the Spark UI and note that this operation does not generate a *Spark\n",
    "# job*.\n",
    "\n",
    "# Get the number of partitions:\n",
    "rides.rdd.getNumPartitions()\n",
    "\n",
    "# Note that we are accessing the [Resilient Distributed Dataset\n",
    "# (RDD)](http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "# underlying the DataFrame.\n",
    "\n",
    "# Print the schema:\n",
    "rides.printSchema()\n",
    "\n",
    "# Note that this operation also does not generate a job.\n",
    "\n",
    "# Print a few rows:\n",
    "rides.show(5)\n",
    "\n",
    "# Note that `show` is an *action* and generates a job with one *stage* and one\n",
    "# *task*.  Show is actually a *partial action* since Spark does not have to\n",
    "# read all the data to print out a few rows.\n",
    "\n",
    "# `count` is also an action:\n",
    "rides.count()\n",
    "\n",
    "# Note that `count` generates a job with two stages and three tasks.  The first\n",
    "# stage consists of two parallel tasks that count the number of rows in each\n",
    "# partition.  The second stage consists of one task that adds these partial sum\n",
    "# to compute the final count.\n",
    "\n",
    "# Save the DataFrame to HDFS:\n",
    "rides.write.mode(\"overwrite\").text(\"data/monitor/\")\n",
    "\n",
    "# Note that each partition is written to a separate file.\n",
    "\n",
    "# Let us repartition the DataFrame into six partitions:\n",
    "rides6 = rides.repartition(6)\n",
    "\n",
    "# Count the number of rows:\n",
    "rides6.count()\n",
    "\n",
    "# Note that repartitioning the DataFrame requires shuffling data and therefore\n",
    "# generates an additional stage.\n",
    "\n",
    "# The `coalesce` method is a more efficient way to reduce the number of\n",
    "# partitions:\n",
    "rides.coalesce(1).write.mode(\"overwrite\").text(\"data/monitor/\")\n",
    "\n",
    "# Here we have used the `coalesce` method to reduce the number of partitions\n",
    "# before writing the DataFrame to HDFS.\n",
    "\n",
    "# Remove the temporary file:\n",
    "!hdfs dfs -rm -r data/monitor/\n",
    "\n",
    "\n",
    "# ### Example 2: Persisting DataFrames\n",
    "\n",
    "# Read the ride data as a (comma) delimited text file:\n",
    "rides = spark.read.csv(\"/duocar/raw/rides\", header=True, inferSchema=True)\n",
    "\n",
    "# Note that Spark ran two exploratory jobs to read the header and infer the\n",
    "# schema.\n",
    "\n",
    "# Duplicate the ride data to make it bigger:\n",
    "big_rides = spark.range(100).crossJoin(rides)\n",
    "\n",
    "# Print the number of partitions:\n",
    "big_rides.rdd.getNumPartitions()\n",
    "\n",
    "# Chain together a more elaborate set of transformations:\n",
    "from pyspark.sql.functions import count, mean, stddev\n",
    "result = big_rides \\\n",
    "  .groupby(\"rider_id\") \\\n",
    "  .agg(count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"count(distance)\", ascending=False)\n",
    "\n",
    "# Spark determines the appropriate number of partitions:\n",
    "result.rdd.getNumPartitions()\n",
    "\n",
    "# Persist the DataFrame in memory:\n",
    "result.persist()\n",
    "\n",
    "# Review the **Storage** tab in the Spark UI.  Spark does not persist the DataFrame\n",
    "# until it is actually computed.\n",
    "\n",
    "# Run an action to compute the DataFrame:\n",
    "%time result.count()\n",
    "\n",
    "# Note that the DataFrame is now listed under the **Storage** tab in the Spark UI.\n",
    "\n",
    "# Run the action again:\n",
    "%time result.count()\n",
    "\n",
    "# Note that it runs noticeably faster since the result is already in memory:\n",
    "\n",
    "# Free up memory:\n",
    "result.unpersist()\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n",
    "\n",
    "# This also stops the Spark Application and disables the Spark UI.\n",
    "\n",
    "\n",
    "# ## Configuring the Spark Environment\n",
    "\n",
    "# We have been creating a SparkSession using the following syntax:\n",
    "#```python\n",
    "#spark = SparkSession.builder \\\n",
    "#  .master(\"local\") \\\n",
    "#  .appName(\"config\") \\\n",
    "#  .getOrCreate()\n",
    "#```\n",
    "\n",
    "# This is actually a special case of the following more general syntax:\n",
    "#```python\n",
    "#spark = SparkSession.builder \\\n",
    "#  .config(\"spark.master\", \"local\") \\\n",
    "#  .config(\"spark.app.name\", \"config\") \\\n",
    "#  .getOrCreate()\n",
    "#```\n",
    "\n",
    "# We can configure additional environment settings:\n",
    "spark = SparkSession.builder \\\n",
    "  .config(\"spark.master\", \"local\") \\\n",
    "  .config(\"spark.app.name\", \"config\") \\\n",
    "  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# We can query a configuration property using the following syntax:\n",
    "spark.conf.get(\"spark.driver.memory\")\n",
    "\n",
    "# We can view other settings under the **Environment** tab of the Spark UI.\n",
    "\n",
    "# You can set configuration properties in the `spark-defaults.conf` file:\n",
    "!cat spark-defaults.conf\n",
    "\n",
    "# Stop the SparkSession (and the Spark application):\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "# [Monitoring Spark\n",
    "# Applications](https://docs.cloudera.com/documentation/enterprise/latest/topics/operation_spark_applications.html#spark_monitoring)\n",
    "\n",
    "# [Tuning Spark\n",
    "# Applications](https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_spark_tuning1.html)\n",
    "\n",
    "# [Configuring the Cloudera Distribution of Apache Spark\n",
    "# 2](https://docs.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_spark_configuration.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
