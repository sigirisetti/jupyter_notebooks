{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a784d749-d60d-4843-a46d-4ebf0813db9c",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d1183-bcb6-4a0b-83fe-1d004bfb8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transforming DataFrames\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate some basic transformations on DataFrames:\n",
    "# * Working with columns\n",
    "#   * Selecting columns\n",
    "#   * Dropping columns\n",
    "#   * Specifying columns\n",
    "#   * Adding columns\n",
    "#   * Changing the column name\n",
    "#   * Changing the column type\n",
    "# * Working with rows\n",
    "#   * Ordering rows\n",
    "#   * Keeping a fixed number of rows\n",
    "#   * Keeping distinct rows\n",
    "#   * Filtering rows\n",
    "#   * Sampling rows\n",
    "# * Working with missing values\n",
    "\n",
    "# **Note:** There is often more than one way to do these transformations.\n",
    "\n",
    "\n",
    "# ## Spark SQL DataFrames\n",
    "\n",
    "# * Spark SQL DataFrames are inspired by R data frames and Python pandas DataFrames\n",
    "\n",
    "# * Spark SQL DataFrames are resilient distributed structured tables\n",
    "#   * A DataFrame is a distributed collection of *Row* objects\n",
    "#   * A Row object is an ordered collection of objects with names and types\n",
    "\n",
    "# * Properties of DataFrames\n",
    "#   * Immutable - DataFrames are read-only\n",
    "#   * Evaluated lazily - Spark only does work when it has to\n",
    "#   * Ephemeral - DataFrames disappear unless explicitly persisted\n",
    "\n",
    "# * Operations on DataFrames\n",
    "#   * *Transformations* return a DataFrame\n",
    "#     * Narrow Transformations\n",
    "#     * Wide Transformations\n",
    "#   * *Actions* cause Spark to do work\n",
    "\n",
    "# * Spark SQL provides two APIs\n",
    "#   * SQL\n",
    "#   * DataFrame\n",
    "\n",
    "# * Spark SQL code is declarative - the Catalyst optimizer produces core Spark RDD code\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"dataframes\").getOrCreate()\n",
    "\n",
    "# Read the raw data from HDFS:\n",
    "\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", header=True, inferSchema=True)\n",
    "rides.printSchema()\n",
    "\n",
    "drivers = spark.read.csv(\"/duocar/raw/drivers/\", header=True, inferSchema=True)\n",
    "drivers.printSchema()\n",
    "\n",
    "riders = spark.read.csv(\"/duocar/raw/riders/\", header=True, inferSchema=True)\n",
    "riders.printSchema()\n",
    "\n",
    "\n",
    "# ## Working with Columns\n",
    "\n",
    "# ### Selecting Columns\n",
    "\n",
    "# Use the `select` method to select specific columns:\n",
    "riders.select(\"birth_date\", \"student\", \"sex\").printSchema()\n",
    "\n",
    "# ### Dropping Columns\n",
    "\n",
    "# Use the `drop` method to drop specific columns:\n",
    "riders.drop(\"first_name\", \"last_name\", \"ethnicity\").printSchema()\n",
    "\n",
    "# ### Specifying Columns\n",
    "\n",
    "# We have used the column name to reference a DataFrame column:\n",
    "riders.select(\"first_name\").printSchema()\n",
    "\n",
    "# We actually work with `Column` objects in Spark SQL.\n",
    "# Use the following syntax to reference a column object in a particular DataFrame:\n",
    "riders.select(riders.first_name).printSchema()\n",
    "riders.select(riders[\"first_name\"]).printSchema()\n",
    "type(riders[\"first_name\"])\n",
    "\n",
    "# Use the `col` or `column` function to reference a general column object:\n",
    "from pyspark.sql.functions import col, column\n",
    "riders.select(col(\"first_name\")).printSchema()\n",
    "riders.select(column(\"first_name\")).printSchema()\n",
    "type(column(\"first_name\"))\n",
    "\n",
    "# Use `*` (in quotes) to specify all columns:\n",
    "riders.select(\"*\").printSchema()\n",
    "\n",
    "# ### Adding Columns\n",
    "\n",
    "# Use the `withColumn` method to add a new column:\n",
    "riders \\\n",
    "  .select(\"student\") \\\n",
    "  .withColumn(\"student_boolean\", col(\"student\") == 1) \\\n",
    "  .show()\n",
    "\n",
    "# The `select` method also works:\n",
    "riders.select(\"student\", (col(\"student\") == 1).alias(\"student_boolean\")).show(5)\n",
    "\n",
    "# The `selectExpr` method accepts partial SQL expressions:\n",
    "riders.selectExpr(\"student\", \"student = 1 as student_boolean\").show(5)\n",
    "\n",
    "# The `sql` method accepts full SQL statements:\n",
    "riders.createOrReplaceTempView(\"riders_view\")\n",
    "spark.sql(\"select student, student = 1 as student_boolean from riders_view\").show(5)\n",
    "\n",
    "# ### Changing the column name\n",
    "\n",
    "# Use the `withColumnRenamed` method to rename a column:\n",
    "riders.withColumnRenamed(\"start_date\", \"join_date\").printSchema()\n",
    "\n",
    "# Chain multiple methods to rename more than one column:\n",
    "riders \\\n",
    "  .withColumnRenamed(\"start_date\", \"join_date\") \\\n",
    "  .withColumnRenamed(\"sex\", \"gender\") \\\n",
    "  .printSchema()\n",
    "\n",
    "# ### Changing the column type\n",
    "\n",
    "# Recall that `home_block` was read in as a (long) integer:\n",
    "riders.printSchema()\n",
    "\n",
    "# Use the `withColumn` (DataFrame) method in conjunction with the `cast`\n",
    "# (Column) method to change its type:\n",
    "riders.withColumn(\"home_block\", col(\"home_block\").cast(\"string\")).printSchema()\n",
    "\n",
    "# **Note:** If we need to change the name and/or type of many columns, then we\n",
    "# may want to consider specifying the schema on read.\n",
    "\n",
    "\n",
    "# ## Working with rows\n",
    "\n",
    "# ### Ordering rows\n",
    "\n",
    "# Use the `sort` or `orderBy` method to sort a DataFrame by particular columns:\n",
    "rides \\\n",
    "  .select(\"rider_id\", \"date_time\") \\\n",
    "  .sort(\"rider_id\", \"date_time\", ascending=True) \\\n",
    "  .show()\n",
    "\n",
    "# **Note:** Ascending order is the default.\n",
    "\n",
    "rides \\\n",
    "  .select(\"rider_id\", \"date_time\") \\\n",
    "  .orderBy(\"rider_id\", \"date_time\", ascending=False) \\\n",
    "  .show()\n",
    "\n",
    "# Use the `asc` and `desc` methods to specify the sort order:\n",
    "rides \\\n",
    "  .select(\"rider_id\", \"date_time\") \\\n",
    "  .sort(col(\"rider_id\").asc(), col(\"date_time\").desc()) \\\n",
    "  .show()\n",
    "\n",
    "# Alternatively, use the `asc` and `desc` functions to specify the sort order:\n",
    "from pyspark.sql.functions import asc, desc\n",
    "rides \\\n",
    "  .select(\"rider_id\", \"date_time\") \\\n",
    "  .orderBy(asc(\"rider_id\"), desc(\"date_time\")) \\\n",
    "  .show()\n",
    "\n",
    "# ### Selecting a fixed number of rows\n",
    "\n",
    "# Use the `limit` method to select a fixed number of rows:\n",
    "riders.select(\"student\", \"sex\").limit(5).show()\n",
    "\n",
    "# **Question:** What is the difference between `df.show(5)` and `df.limit(5).show()`?\n",
    "\n",
    "# ### Selecting distinct rows\n",
    "\n",
    "# Use the `distinct` method to select distinct rows:\n",
    "riders.select(\"student\", \"sex\").distinct().show()\n",
    "\n",
    "# You can also use the `dropDuplicates` method:\n",
    "riders.select(\"student\", \"sex\").dropDuplicates().show()\n",
    "\n",
    "# ### Filtering rows\n",
    "\n",
    "# Use the `filter` or `where` method along with a Boolean column expression to select\n",
    "# particular rows:\n",
    "riders.filter(col(\"student\") == 1).count()\n",
    "riders.where(col(\"sex\") == \"female\").count()\n",
    "riders.filter(col(\"student\") == 1).where(col(\"sex\") == \"female\").count()\n",
    "\n",
    "# ### Sampling rows\n",
    "\n",
    "# Use the `sample` method to select a random sample of rows with or without\n",
    "# replacement:\n",
    "riders.count()\n",
    "riders.sample(withReplacement=False, fraction=0.1, seed=12345).count()\n",
    "\n",
    "# Use the `sampleBy` method to select a stratified random sample:\n",
    "riders \\\n",
    "  .groupBy(\"sex\") \\\n",
    "  .count() \\\n",
    "  .show() \n",
    "riders \\\n",
    "  .sampleBy(\"sex\", fractions={\"male\": 0.2, \"female\": 0.8}, seed=54321) \\\n",
    "  .groupBy(\"sex\") \\\n",
    "  .count() \\\n",
    "  .show()\n",
    "\n",
    "# We have randomly sampled 20% of the male riders and 80% of the female riders.\n",
    "\n",
    "\n",
    "# ## Working with missing values\n",
    "\n",
    "# Note the missing (null) values in the following DataFrame:\n",
    "riders_selected = riders.select(\"id\", \"sex\", \"ethnicity\")\n",
    "riders_selected.show(25)\n",
    "\n",
    "# Drop rows with any missing values:\n",
    "riders_selected.dropna(how=\"any\", subset=[\"sex\", \"ethnicity\"]).show(25)\n",
    "\n",
    "# Drop rows with all missing values:\n",
    "riders_selected.na.drop(how=\"all\", subset=[\"sex\", \"ethnicity\"]).show(25)\n",
    "\n",
    "# **Note**: `dropna` and `na.drop` are equivalent.\n",
    "\n",
    "# Replace missing values with a common value:\n",
    "riders_selected.fillna(\"OTHER/UNKNOWN\", [\"sex\", \"ethnicity\"]).show(25)\n",
    "\n",
    "# Replace missing values with different values:\n",
    "riders_missing = riders_selected.na.fill({\"sex\": \"OTHER/UNKNOWN\", \"ethnicity\": \"MISSING\"})\n",
    "riders_missing.show(25)\n",
    "\n",
    "# **Note**: `fillna` and `na.fill` are equivalent.\n",
    "\n",
    "# **Note**: The fill value type must match the column type.\n",
    "\n",
    "# Replace arbitrary values with a common value:\n",
    "riders_missing.replace([\"OTHER/UNKNOWN\", \"MISSING\"], \"NA\", [\"sex\", \"ethnicity\"]).show(25)\n",
    "\n",
    "# Replace arbitrary values with different values:\n",
    "riders_missing.na.replace({\"OTHER/UNKNOWN\": \"NA\", \"MISSING\": \"NO RESPONSE\"}, None, [\"sex\", \"ethnicity\"]).show(25)\n",
    "\n",
    "# **Note:** `replace` and `na.replace` are equivalent.\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Replace the missing values in `rides.service` with the string `Car`.\n",
    "\n",
    "# (2) Rename `rides.cancelled` to `rides.canceled`.\n",
    "\n",
    "# (3) Sort the `rides` DataFrame in descending order with respect to\n",
    "# `driver_id` and ascending order with respect to `date_time`.\n",
    "\n",
    "# (4) Create an approximate 20% random sample of the `rides` DataFrame.\n",
    "\n",
    "# (5) Remove the driver's name from the `drivers` DataFrame.\n",
    "\n",
    "# (6) How many drivers have signed up?  How many female drivers have signed up?\n",
    "# How many non-white, female drivers have signed up?\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrame\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrameNaFunctions\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.Column\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.functions\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
