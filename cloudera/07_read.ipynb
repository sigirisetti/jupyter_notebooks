{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c17e2e-f5df-4617-9bd0-23a86b52b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading and Writing DataFrames\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we introduce the DataFrameReader and DataFrameWriter classes\n",
    "# and demonstrate how to read from and write to a number of data sources.\n",
    "\n",
    "\n",
    "# ## Reading and Writing Data\n",
    "\n",
    "# * Spark can read from and write to a variety of data sources.\n",
    "\n",
    "# * The Spark SQL `DataFrameReader` and `DataFrameWriter` classes support the\n",
    "# following data sources:\n",
    "#   * text\n",
    "#   * delimited text\n",
    "#   * JSON (JavaScript Object Notation)\n",
    "#   * Apache Parquet\n",
    "#   * Apache ORC\n",
    "#   * Apache Hive\n",
    "#   * JDBC connection\n",
    "\n",
    "# * Spark SQL also integrates with the pandas Python package.\n",
    "\n",
    "# * Additional data sources are supported by [third-party\n",
    "# packages](https://spark-packages.org/).\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"read\") \\\n",
    "  .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# This configuration parameter \n",
    "# `spark.hadoop.fs.s3a.aws.credentials.provider`\n",
    "# is required to read data from\n",
    "# a public Amazon S3 bucket in the section below entitled\n",
    "# **Working with object stores**.\n",
    "\n",
    "# Create an HDFS directory for saved data:\n",
    "!hdfs dfs -rm -r -skipTrash data  # Remove any existing directory\n",
    "!hdfs dfs -mkdir data\n",
    "\n",
    "\n",
    "# ## Working with delimited text files\n",
    "\n",
    "# Use the\n",
    "# [csv](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
    "# method of the\n",
    "# [DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)\n",
    "# class to read a delimited text file:\n",
    "riders = spark \\\n",
    "  .read \\\n",
    "  .csv(\"/duocar/raw/riders/\", sep=\",\", header=True, inferSchema=True) \\\n",
    "\n",
    "# The `csv` method is a convenience method for the following more general\n",
    "# syntax:\n",
    "riders = spark \\\n",
    "  .read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .load(\"/duocar/raw/riders/\")\n",
    "\n",
    "# **Note:** If you use either syntax with `header` set to `True`, then Spark\n",
    "# assumes that *every* file in the directory has a header row.\n",
    "\n",
    "# Spark does its best to infer the schema from the header row and column\n",
    "# values:\n",
    "riders.printSchema()\n",
    "\n",
    "# Alternatively, you can manually specify the schema.  First, import the Spark\n",
    "# SQL\n",
    "# [types](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n",
    "# module:\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Then specify the schema as a `StructType` instance: \n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"birth_date\", DateType(), True),\n",
    "    StructField(\"join_date\", DateType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"ethnicity\", StringType(), True),\n",
    "    StructField(\"student\", IntegerType(), True),\n",
    "    StructField(\"home_block\", StringType(), True),\n",
    "    StructField(\"home_lat\", DoubleType(), True),\n",
    "    StructField(\"home_lon\", DoubleType(), True),\n",
    "    StructField(\"work_lat\", DoubleType(), True),\n",
    "    StructField(\"work_lon\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Finally, pass the schema to the `DataFrameReader`:\n",
    "riders2 = spark \\\n",
    "  .read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .schema(schema) \\\n",
    "  .load(\"/duocar/raw/riders/\")\n",
    "\n",
    "# **Note:** We must include the header option otherwise Spark will read the\n",
    "# header row as a valid record.\n",
    "\n",
    "# Confirm the explicit schema:\n",
    "riders2.printSchema()\n",
    "\n",
    "# Use the `csv` method of the\n",
    "# [DataFrameWriter](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter)\n",
    "# class to write the DataFrame to a tab-delimited file:\n",
    "riders2.write.csv(\"data/riders_tsv/\", sep=\"\\t\")\n",
    "!hdfs dfs -ls data/riders_tsv\n",
    "\n",
    "# **Note:** The file has a `csv` extension even though it includes\n",
    "# tab-separated values.  Never trust a file extension!\n",
    "\n",
    "# Use the `mode` argument to overwrite existing files and the `compression`\n",
    "# argument to specify a compression codec:\n",
    "riders2.write.csv(\"data/riders_tsv_compressed/\", sep=\"\\t\", mode=\"overwrite\", compression=\"bzip2\")\n",
    "!hdfs dfs -ls data/riders_tsv_compressed\n",
    "\n",
    "# See the Cloudera documentation on [Data\n",
    "# Compression](https://docs.cloudera.com/documentation/enterprise/latest/topics/introduction_compression.html)\n",
    "# for more details.\n",
    "\n",
    "\n",
    "# ## Working with text files\n",
    "\n",
    "# Use the\n",
    "# [text](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.text)\n",
    "# method of the `DataFrameReader` class to read an unstructured text file:\n",
    "weblogs = spark.read.text(\"/duocar/earcloud/apache_logs/\")\n",
    "weblogs.printSchema()\n",
    "weblogs.head(5)\n",
    "\n",
    "# **Note:** The default filesystem in Hadoop (and by extension CDSW) is HDFS.\n",
    "# The read statement above is a shortcut for\n",
    "#```python\n",
    "#weblogs = spark.read.text(\"hdfs:///duocar/earcloud/apache_logs/\")\n",
    "#```\n",
    "# which in turn is a shortcut for\n",
    "#```python\n",
    "#weblogs = spark.read.text(\"hdfs:/<host:port>//duocar/earcloud/apache_logs\")\n",
    "#```\n",
    "# where `<host:port>` is the host and port of the HDFS namenode.\n",
    "\n",
    "# Parse the unstructured data:\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "requests = weblogs.select(regexp_extract(\"value\", \"^.*\\\"(GET.*?)\\\".*$\", 1).alias(\"request\")) \n",
    "requests.head(5)\n",
    "\n",
    "# Use the\n",
    "# [text](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.text)\n",
    "# method of the `DataFrameWriter` class to write an unstructured text file:\n",
    "requests.write.text(\"data/requests_txt/\")\n",
    "!hdfs dfs -ls data/requests_txt\n",
    "\n",
    "\n",
    "# ## Working with Parquet files\n",
    "\n",
    "# [Parquet](https://parquet.apache.org/) is a very popular columnar storage\n",
    "# format for Hadoop.  Parquet is the default file format in Spark SQL.  Use\n",
    "# the `parquet` method of the `DataFrameWriter` class to write to a Parquet\n",
    "# file:\n",
    "riders2.write.parquet(\"data/riders_parquet/\")\n",
    "!hdfs dfs -ls data/riders_parquet\n",
    "\n",
    "# **Note:** The SLF4J messages are a known issue with CDH.  You can safely\n",
    "# ignore them.\n",
    "\n",
    "# Use the `parquet` method of the `DataFrameReader` class to the read from a\n",
    "# Parquet file:\n",
    "spark.read.parquet(\"data/riders_parquet/\").printSchema()\n",
    "\n",
    "# **Note:** Spark uses the schema stored with the data.\n",
    "\n",
    "\n",
    "# ## Working with Hive Tables\n",
    "\n",
    "# Use the `sql` method of the `SparkSession` class to run Hive queries:\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE duocar\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "spark.sql(\"DESCRIBE riders\").show()\n",
    "spark.sql(\"SELECT * FROM riders LIMIT 10\").show()\n",
    "\n",
    "# Use the `table` method of the `DataFrameReader` class to read a Hive table:\n",
    "riders_table = spark.read.table(\"riders\")\n",
    "riders_table.printSchema()\n",
    "riders_table.show(5)\n",
    "\n",
    "# Use the `saveAsTable` method of the `DataFrameWriter` class to write a Hive\n",
    "# table:\n",
    "import uuid\n",
    "table_name = \"riders_\" + str(uuid.uuid4().hex)  # Create unique table name.\n",
    "riders.write.saveAsTable(table_name)\n",
    "\n",
    "\n",
    "# You can now manipulate this table with Hive or Impala or via Spark SQL:\n",
    "spark.sql(\"DESCRIBE %s\" % table_name).show()\n",
    "\n",
    "\n",
    "# ## Working with object stores\n",
    "\n",
    "# Pass the appropriate prefix and path to the DataFrameReader and\n",
    "# DataFrameWriter methods to read from and write to an object store.  For\n",
    "# example, use the prefix `s3a` and pass the S3 bucket to read from Amazon S3:\n",
    "\n",
    "demographics = spark.read.csv(\"s3a://duocar/raw/demographics/\", sep=\"\\t\", header=True, inferSchema=True)\n",
    "demographics.printSchema()\n",
    "demographics.show(5)\n",
    "\n",
    "# If we have write permissions, then we can also write files to Amazon S3 using\n",
    "# the `s3a` prefix.\n",
    "\n",
    "# **Important:** This code will fail when running Spark via YARN unless the\n",
    "# worker nodes have access to the appropriate AWS credentials.  See the\n",
    "# documentation for your distribution of Hadoop for more details on accessing\n",
    "# cloud storage.\n",
    "\n",
    "\n",
    "# ## Working with pandas DataFrames\n",
    "\n",
    "# Import the pandas package:\n",
    "import pandas as pd\n",
    "\n",
    "# Use the pandas `read_csv` method to read a local tab-delimited file:\n",
    "demographics_pdf = pd.read_csv(\"data/demographics.txt\", sep=\"\\t\")\n",
    "\n",
    "# Access the pandas `dtypes` attribute to the view the data types:\n",
    "demographics_pdf.dtypes\n",
    "\n",
    "# Use the pandas `head` method to view the data:\n",
    "demographics_pdf.head()\n",
    "\n",
    "# Use the `createDataFrame` method of the `SparkSession` class to create a Spark\n",
    "# DataFrame from a pandas DataFrame:\n",
    "demographics = spark.createDataFrame(demographics_pdf)\n",
    "demographics.printSchema()\n",
    "demographics.show(5)\n",
    "\n",
    "# Use the `toPandas` method to read a Spark DataFrame into a pandas DataFrame:\n",
    "riders_pdf = riders.toPandas()\n",
    "riders_pdf.dtypes\n",
    "riders_pdf.head()\n",
    "\n",
    "# **WARNING:** Use this with caution as you may use all your available memory!\n",
    "\n",
    "# **Note:** Column types may not convert as expected when reading a Spark\n",
    "# DataFrame into a pandas DataFrame and vice versa.\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Use the `json` method of the `DataFrameWriter` class to write the\n",
    "# `riders` DataFrame to the `data/riders_json/` (HDFS) directory.\n",
    "\n",
    "riders2.write.json(\"data/riders_json\",mode=\"overwrite\")\n",
    "\n",
    "# (2) Use the `hdfs dfs -ls` command to list the contents of the\n",
    "# `data/riders_json/` directory.\n",
    "\n",
    "!hdfs dfs -ls data/riders_json\n",
    "\n",
    "# (3) Use the `hdfs dfs -cat` and `head` commands to display a JSON file in\n",
    "# the `data/riders_json` directory.\n",
    "\n",
    "!hdfs dfs -cat data/riders_json/part* | head -n 5\n",
    "\n",
    " \n",
    "# (4) Use Hue to browse the `data/riders_json/` directory.\n",
    "\n",
    "# (5) Use the `json` method of the `DataFrameReader` class to read the JSON\n",
    "# file into a DataFrame.\n",
    "riders_json = spark.read.json(\"data/riders_json/\")\n",
    "\n",
    "# (6) Examine the schema of the DataFrame.  Do you notice anything different?\n",
    "riders_json.printSchema()\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrameReader\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.DataFrameWriter\n",
    "# class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Drop the Hive table:\n",
    "spark.sql(\"DROP TABLE IF EXISTS %s\" % table_name)\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
