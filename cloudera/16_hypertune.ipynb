{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a1823-275d-41cc-ab7b-3e26ee17f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tuning Algorithm Hyperparameters Using Grid Search\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Introduction\n",
    "\n",
    "# Most machine learning algorithms have a set of user-specified parameters that\n",
    "# govern the behavior of the algorithm.  These parameters are called\n",
    "# *hyperparameters* to distinguish them from the model parameters such as the\n",
    "# intercept and coefficients in linear and logistic regression.  In this module\n",
    "# we show how to use grid search and cross validation in Spark MLlib to\n",
    "# determine a reasonable regularization parameter for [$l1$ lasso linear\n",
    "# regression](https://en.wikipedia.org/wiki/Lasso_%28statistics%29).\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ## Create a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"hypertune\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Load the data\n",
    "\n",
    "# **Important:**  Run the `15_classify.py` script before loading the data.\n",
    "\n",
    "# Read the modeling data from HDFS:\n",
    "rides = spark.read.parquet(\"data/modeling_data\")\n",
    "rides.show(5)\n",
    "\n",
    "\n",
    "# ## Create train and test data\n",
    "\n",
    "(train, test) = rides.randomSplit([0.7, 0.3], 12345)\n",
    "\n",
    "\n",
    "# ## Requirements for hyperparameter tuning\n",
    "\n",
    "# We need to specify four components to perform hyperparameter tuning using\n",
    "# grid search:\n",
    "# * Estimator (i.e. machine learning algorithm)\n",
    "# * Hyperparameter grid\n",
    "# * Evaluator\n",
    "# * Validation method\n",
    "\n",
    "\n",
    "# ## Specify the estimator\n",
    "\n",
    "# In this example we will use $l1$ (lasso) linear regression as our estimator:\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"star_rating\", elasticNetParam=1.0)\n",
    "\n",
    "# Use the `explainParams` method to get the full list of hyperparameters:\n",
    "print(lr.explainParams())\n",
    "\n",
    "# Setting `elasticNetParam=1.0` corresponds to $l1$ (lasso) linear regression.\n",
    "# We are interested in finding a reasonable value of `regParam`.\n",
    "\n",
    "\n",
    "# ## Specify the hyperparameter grid\n",
    "\n",
    "# Use the\n",
    "# [ParamGridBuilder](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder)\n",
    "# class to specify a hyperparameter grid:\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "regParamList = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "grid = ParamGridBuilder().addGrid(lr.regParam, regParamList).build()\n",
    "\n",
    "# The resulting object is simply a list of parameter maps:\n",
    "grid\n",
    "\n",
    "# Rather than specify `elasticNetParam` in the `LinearRegression` constructor, we can specify it in our grid:\n",
    "grid = ParamGridBuilder().baseOn({lr.elasticNetParam: 1.0}).addGrid(lr.regParam, regParamList).build()\n",
    "grid\n",
    "\n",
    "\n",
    "# ## Specify the evaluator\n",
    "\n",
    "# In this case we will use\n",
    "# [RegressionEvaluator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator)\n",
    "# as our evaluator and specify root-mean-squared error as the metric:\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"star_rating\", metricName=\"rmse\")\n",
    "\n",
    "\n",
    "# ## Tune the hyperparameters using holdout cross-validation\n",
    "\n",
    "# In most cases, holdout cross-validation will be sufficient.  Use the\n",
    "# [TrainValidationSplit](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.TrainValidationSplit)\n",
    "# class to specify holdout cross-validation:\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, trainRatio=0.75, seed=54321)\n",
    "\n",
    "# For each combination of the hyperparameters, the linear regression will be\n",
    "# fit on a random %75 of the `train` DataFrame and evaluated on the remaining\n",
    "# %25. \n",
    "\n",
    "# Use the `fit` method to find the best set of hyperparameters:\n",
    "%time tvs_model = tvs.fit(train)\n",
    "\n",
    "# The resulting model is an instance of the\n",
    "# [TrainValidationSplitModel](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.TrainValidationSplitModel)\n",
    "# class:\n",
    "type(tvs_model)\n",
    "\n",
    "# The cross-validation results are stored in the `validationMetrics` attribute:\n",
    "tvs_model.validationMetrics\n",
    "\n",
    "# These are the RMSE for each set of hyperparameters.  Smaller is better.\n",
    "\n",
    "def plot_holdout_results(model):\n",
    "  plt.plot(regParamList, model.validationMetrics)\n",
    "  plt.title(\"Hyperparameter Tuning Results\")\n",
    "  plt.xlabel(\"Regularization Parameter\")\n",
    "  plt.ylabel(\"Validation Metric\")\n",
    "  plt.show()\n",
    "plot_holdout_results(tvs_model)\n",
    "\n",
    "# In this case the `bestModel` attribute is an instance of the\n",
    "# [LinearRegressionModel](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegressionModel)\n",
    "# class:\n",
    "type(tvs_model.bestModel)\n",
    "\n",
    "# **Note:** The model is rerun on the entire train dataset using the best set of hyperparameters.\n",
    "\n",
    "# The usual attributes and methods are available:\n",
    "tvs_model.bestModel.intercept\n",
    "tvs_model.bestModel.coefficients\n",
    "tvs_model.bestModel.summary.rootMeanSquaredError\n",
    "tvs_model.bestModel.evaluate(test).r2\n",
    "\n",
    "\n",
    "# ## Tune the hyperparameters using k-fold cross-validation\n",
    "\n",
    "# For small or noisy datasets, k-fold cross-validation may be more appropriate.\n",
    "# Use the\n",
    "# [CrossValidator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n",
    "# class to specify the k-fold cross-validation:\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3, seed=54321)\n",
    "%time cv_model = cv.fit(train)\n",
    "\n",
    "# The result is an instance of the\n",
    "# [CrossValidatorModel](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidatorModel)\n",
    "# class:\n",
    "type(cv_model)\n",
    "\n",
    "# The cross-validation results are stored in the `avgMetrics` attribute:\n",
    "cv_model.avgMetrics\n",
    "def plot_kfold_results(model):\n",
    "  plt.plot(regParamList, model.avgMetrics)\n",
    "  plt.title(\"Hyperparameter Tuning Results\")\n",
    "  plt.xlabel(\"Regularization Parameter\")\n",
    "  plt.ylabel(\"Validation Metric\")\n",
    "  plt.show()\n",
    "plot_kfold_results(cv_model)\n",
    "\n",
    "# The `bestModel` attribute contains the model based on the best set of\n",
    "# hyperparameters.  In this case, it is an instance of the\n",
    "# `LinearRegressionModel` class:\n",
    "type(cv_model.bestModel)\n",
    "\n",
    "# Compute the performance of the performance of the best model on the test\n",
    "# dataset:\n",
    "cv_model.bestModel.evaluate(test).r2\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Maybe our regularization parameters are too large.  Rerun the\n",
    "# hyperparameter tuning with regularization parameters [0.0, 0.002, 0.004, 0.006,\n",
    "# 0.008, 0.01].\n",
    "\n",
    "# (2) Create a parameter grid that searches over `elasticNetParam` as well as\n",
    "# `regParam`.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Documentation - Model Selection and hyperparameter\n",
    "# tuning](http://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "\n",
    "# [Spark Python API - pyspark.ml.tuning\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.tuning)\n",
    "\n",
    "\n",
    "# ## Stop the SparkSession\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
