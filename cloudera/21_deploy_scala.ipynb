{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072ecb5-22ca-4e44-937f-f4ea4ef4e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* # Loading and Using Machine Learning Pipelines in Scala */\n",
    "\n",
    "/* Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "   Not to be reproduced or shared without prior written \n",
    "   consent from Cloudera. */\n",
    "\n",
    "\n",
    "/*\n",
    "## Overview\n",
    "*/\n",
    "\n",
    "/*\n",
    "In this module we demonstrate how to load and use the pipeline model object\n",
    "that we saved in our Python session.  In addition, we give you a glimpse of the\n",
    "Spark Scala API.  You will find that it looks quite similar to the Python API.\n",
    "*/\n",
    "\n",
    "/*\n",
    "**Note**: // comments do not render Markdown.\n",
    "*/\n",
    "\n",
    "\n",
    "/*\n",
    "## Create a SparkSession\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"deploy\").getOrCreate()\n",
    "\n",
    "/*\n",
    "**Note**:  We do not have to create a SparkSession in a Scala session because\n",
    "it is automatically created for us; however, doing so will allow us to the run\n",
    "this script as a standalone Spark application.\n",
    "*/\n",
    "spark\n",
    "\n",
    "/*\n",
    "Note that Spark is running in yarn client mode by default:\n",
    "*/\n",
    "spark.conf.get(\"spark.master\")\n",
    "spark.conf.get(\"spark.submit.deployMode\")\n",
    "\n",
    "\n",
    "/*\n",
    "## Load the data\n",
    "*/\n",
    "\n",
    "/*\n",
    "Let us read the enhanced ride data from HDFS:\n",
    "*/\n",
    "val rides = spark.read.parquet(\"/duocar/joined_all/\")\n",
    "\n",
    "/*\n",
    "**Note:** The Spark keyword `val` indicates that `rides` is an immutable\n",
    "object.\n",
    "*/\n",
    "\n",
    "\n",
    "/*\n",
    "## Load a PipelineModel object\n",
    "*/\n",
    "  \n",
    "/*\n",
    "If we simply want to apply our existing classifier to new data, then we load\n",
    "and use our PipelineModel instance:\n",
    "*/\n",
    "import org.apache.spark.ml.PipelineModel\n",
    "val pipelineModel = PipelineModel.load(\"models/pipeline_model\")\n",
    "\n",
    "/*\n",
    "## Inspect the pipeline model\n",
    "*/\n",
    "/*\n",
    "Extract the hyperparameter tuning model:\n",
    "*/\n",
    "import org.apache.spark.ml.tuning.TrainValidationSplitModel\n",
    "val tvsModel = pipelineModel.stages(5).asInstanceOf[TrainValidationSplitModel]\n",
    "\n",
    "/*\n",
    "Extract the random forest classification model:\n",
    "*/\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "val rfModel = tvsModel.bestModel.asInstanceOf[RandomForestClassificationModel]\n",
    "\n",
    "/*\n",
    "Examine the hyperparameters:\n",
    "*/\n",
    "rfModel.getMaxDepth\n",
    "rfModel.getSubsamplingRate\n",
    "rfModel.getNumTrees\n",
    "\n",
    "\n",
    "/*\n",
    "## Apply a PipelineModel object\n",
    "*/\n",
    "\n",
    "/*\n",
    "Use the `transform` method to apply the pipeline model to our new DataFrame:\n",
    "*/\n",
    "val classified = pipelineModel.transform(rides)\n",
    "\n",
    "/*\n",
    "**Important:** The input DataFrame must include the required columns:\n",
    "*/\n",
    "classified.printSchema()\n",
    "\n",
    "\n",
    "/*\n",
    "## Evaluate the pipeline model:\n",
    "*/\n",
    "\n",
    "/*\n",
    "Use the `persist` method to cache our classified DataFrame in (worker) memory:\n",
    "*/\n",
    "classified.persist()\n",
    "\n",
    "/*\n",
    "Verify that the cancelled rides have been removed:\n",
    "*/\n",
    "classified.groupBy(\"cancelled\").count().show()\n",
    "\n",
    "/*\n",
    "Generate the confusion matrix:\n",
    "*/\n",
    "classified.\n",
    "  groupBy(\"prediction\").\n",
    "  pivot(\"star_rating\").\n",
    "  count().\n",
    "  orderBy(\"prediction\").\n",
    "  na.fill(0).\n",
    "  show()\n",
    "\n",
    "/*\n",
    "**Note**: Scala does not provide a `crosstab` method, so we use the `pivot`\n",
    "method instead.\n",
    "*/\n",
    "\n",
    "/*\n",
    "**Note**: We must put the dots at the end of the line rather than the\n",
    "beginning of the line.\n",
    "*/\n",
    "\n",
    "/*\n",
    "Compute the classifier accuracy using the `MulticlassClassificationEvaluator`:\n",
    "*/\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setLabelCol(\"star_rating\").\n",
    "  setMetricName(\"accuracy\")\n",
    "evaluator.evaluate(classified)\n",
    "\n",
    "/*\n",
    "Unpersist the DataFrame:\n",
    "*/\n",
    "classified.unpersist()\n",
    "\n",
    "\n",
    "/*\n",
    "## Exercises\n",
    "\n",
    "None\n",
    "*/\n",
    "\n",
    "\n",
    "/*\n",
    "## References\n",
    "\n",
    "[Spark Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)\n",
    "*/\n",
    "\n",
    "  \n",
    "/*\n",
    "## Stop the SparkSession\n",
    "*/\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
