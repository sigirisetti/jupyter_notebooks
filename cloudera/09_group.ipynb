{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c9679-2906-41fb-a10e-390d690f45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarizing and Grouping DataFrames\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate how to summarize, group, and pivot data in a DataFrame.\n",
    "\n",
    "# * Summarizing data with aggregate functions\n",
    "# * Grouping data\n",
    "# * Pivoting data\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"group\").getOrCreate()\n",
    "\n",
    "# Read the enhanced (joined) ride data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/joined/\")\n",
    "\n",
    "# Since we will be querying the `rides` DataFrame many times, let us persist\n",
    "# it in memory to improve performance:\n",
    "rides.persist()\n",
    "\n",
    "\n",
    "# ## Summarizing data with aggregate functions\n",
    "\n",
    "# Spark provides a number of summarization (aggregate) functions.  For example,\n",
    "# the `describe` method provides basic summary statistics:\n",
    "rides.describe(\"distance\").show()\n",
    "\n",
    "# Use the `count`, `countDistinct`, and `approx_count_distinct` functions to\n",
    "# compute various column counts:\n",
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "rides.select(count(\"*\"), count(\"distance\"), countDistinct(\"distance\"), approx_count_distinct(\"distance\")).show()\n",
    "\n",
    "# **Note:** The `count` function returns the number of rows with non-null values.\n",
    "\n",
    "# **Note:** Use `count(lit(1))` rather than `count(1)` as an alternative to `count(\"*\")`.\n",
    "\n",
    "# The `agg` method returns the same results and can be applied to grouped data:\n",
    "rides.agg(count(\"*\"), count(\"distance\"), countDistinct(\"distance\"), approx_count_distinct(\"distance\")).show()\n",
    "\n",
    "# Use the `sum` and `sumDistinct` functions to compute various column sums:\n",
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "rides.agg(sum(\"distance\"), sumDistinct(\"distance\")).show()\n",
    "\n",
    "# **Question:** When would one use the `sumDistinct` function?\n",
    "\n",
    "# Spark SQL provides a number of summary statistics:\n",
    "from pyspark.sql.functions import mean, stddev, variance, skewness, kurtosis\n",
    "rides.agg(mean(\"distance\"), stddev(\"distance\"), variance(\"distance\"), skewness(\"distance\"), kurtosis(\"distance\")).show()\n",
    "\n",
    "# **Note:** `mean` is an alias for `avg`, `stddev` is an alias for the sample\n",
    "# standard deviation `stddev_samp`, and `variance` is an alias for the sample\n",
    "# variance `var_samp`.  The population standard deviation and population\n",
    "# variance are available via `stddev_pop` and `var_pop`, respectively.\n",
    "\n",
    "# Use the `min` and `max` functions to compute the minimum and maximum, respectively:\n",
    "from pyspark.sql.functions import min, max\n",
    "rides.agg(min(\"distance\"), max(\"distance\")).show()\n",
    "\n",
    "# Use the `first` and `last` functions to compute the first and last values, respectively:\n",
    "from pyspark.sql.functions import first, last\n",
    "rides \\\n",
    "  .orderBy(\"distance\") \\\n",
    "  .agg(first(\"distance\", ignorenulls=False), last(\"distance\", ignorenulls=False)) \\\n",
    "  .show()\n",
    "\n",
    "# **Note:** Null values sort before valid numerical values.\n",
    "\n",
    "# Use the `corr`, `covar_samp`, or `covar_pop` functions to measure the linear\n",
    "# association between two columns:\n",
    "from pyspark.sql.functions import corr, covar_samp, covar_pop\n",
    "rides \\\n",
    "  .agg(corr(\"distance\", \"duration\"), covar_samp(\"distance\", \"duration\"), covar_pop(\"distance\", \"duration\")) \\\n",
    "  .show()\n",
    "\n",
    "# The `collect_list` and `collect_set` functions return a column of array type:\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "rides.agg(collect_set(\"service\")).show(truncate=False)\n",
    "\n",
    "# **Note:** `collect_list` does not remove duplicates and will return a very\n",
    "# long array in this case.\n",
    "\n",
    "\n",
    "# ## Grouping data\n",
    "\n",
    "# Use the `agg` method with the `groupBy` (or `groupby`) method to refine your\n",
    "# analysis:\n",
    "rides \\\n",
    "  .groupBy(\"rider_student\") \\\n",
    "  .agg(count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .show()\n",
    "\n",
    "# You can use more than one column in the `groupBy` method:\n",
    "rides \\\n",
    "  .groupBy(\"rider_student\", \"service\") \\\n",
    "  .agg(count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"rider_student\", \"service\") \\\n",
    "  .show()\n",
    "\n",
    "# Use the `rollup` method to get some subtotals:\n",
    "rides \\\n",
    "  .rollup(\"rider_student\", \"service\") \\\n",
    "  .agg(count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"rider_student\", \"service\") \\\n",
    "  .show()\n",
    "\n",
    "# Use the `grouping` function to identify grouped rows:\n",
    "from pyspark.sql.functions import grouping\n",
    "rides \\\n",
    "  .rollup(\"rider_student\", \"service\") \\\n",
    "  .agg(grouping(\"rider_student\"), grouping(\"service\"), count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"rider_student\", \"service\") \\\n",
    "  .show()\n",
    "\n",
    "# Use the `cube` method to get all subtotals:\n",
    "rides \\\n",
    "  .cube(\"rider_student\", \"service\") \\\n",
    "  .agg(count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"rider_student\", \"service\") \\\n",
    "  .show()\n",
    "\n",
    "# Use the `grouping_id` function to identify grouped rows:\n",
    "from pyspark.sql.functions import grouping_id\n",
    "rides \\\n",
    "  .cube(\"rider_student\", \"service\") \\\n",
    "  .agg(grouping_id(\"rider_student\", \"service\"), count(\"*\"), count(\"distance\"), mean(\"distance\"), stddev(\"distance\")) \\\n",
    "  .orderBy(\"rider_student\", \"service\") \\\n",
    "  .show()\n",
    "\n",
    "\n",
    "# ## Pivoting data\n",
    "\n",
    "# The following use case is common:\n",
    "rides.groupBy(\"rider_student\", \"service\").count().orderBy(\"rider_student\", \"service\").show()\n",
    "\n",
    "# The `crosstab` method can be used to present this result in a pivot table:\n",
    "rides.crosstab(\"rider_student\", \"service\").show()\n",
    "\n",
    "# We can also use the `pivot` method to produce a cross-tabulation:\n",
    "rides.groupBy(\"rider_student\").pivot(\"service\").count().show()\n",
    "\n",
    "# We can also perform other aggregations:\n",
    "rides.groupBy(\"rider_student\").pivot(\"service\").mean(\"distance\").show()\n",
    "\n",
    "rides.groupBy(\"rider_student\").pivot(\"service\").agg(mean(\"distance\")).show()\n",
    "\n",
    "# You can explicitly choose the values that are pivoted to columns:\n",
    "rides.groupBy(\"rider_student\").pivot(\"service\", [\"Car\", \"Grand\"]).agg(mean(\"distance\")).show()\n",
    "\n",
    "# Additional aggregation functions produce additional columns:\n",
    "rides.groupBy(\"rider_student\").pivot(\"service\", [\"Car\"]).agg(count(\"distance\"), mean(\"distance\")).show()\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Who are DuoCar's top 10 riders in terms of number of rides taken?\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"group_solutions\").getOrCreate()\n",
    "\n",
    "# Read the enhanced (joined) ride data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/joined/\")\n",
    "\n",
    "# Since we will be querying the `rides` DataFrame many times, let us persist\n",
    "# it in memory to improve performance:\n",
    "rides.persist()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "rides \\\n",
    "  .filter(col(\"cancelled\") == False) \\\n",
    "  .groupBy(\"rider_id\") \\\n",
    "  .count() \\\n",
    "  .orderBy(col(\"count\").desc()) \\\n",
    "  .show(10)\n",
    "\n",
    "# (2) Who are DuoCar's top 10 drivers in terms of total distance driven?\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "rides \\\n",
    "  .groupBy(\"driver_id\") \\\n",
    "  .agg(sum(\"distance\").alias(\"sum_distance\")) \\\n",
    "  .orderBy(col(\"sum_distance\").desc()) \\\n",
    "  .show(10)\n",
    "\n",
    "# (3) Compute the distribution of cancelled rides.\n",
    "\n",
    "rides.groupBy(\"cancelled\").count().show()\n",
    "\n",
    "# (4) Compute the distribution of ride star rating.\n",
    "# When is the ride star rating missing?\n",
    "\n",
    "rides.groupBy(\"star_rating\").count().orderBy(\"star_rating\").show()\n",
    "\n",
    "# The star rating is missing when a ride is cancelled:\n",
    "rides.crosstab(\"star_rating\", \"cancelled\").orderBy(\"star_rating_cancelled\").show()\n",
    "\n",
    "# (5) Compute the average star rating for each level of car service.\n",
    "# Is the star rating correlated with the level of car service?\n",
    "\n",
    "rides.groupBy(\"service\").mean(\"star_rating\").show()\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Python API - pyspark.sql.functions\n",
    "# module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "# [Spark Python API - pyspark.sql.GroupedData\n",
    "# class](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Unpersist the DataFrame:\n",
    "rides.unpersist()\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
