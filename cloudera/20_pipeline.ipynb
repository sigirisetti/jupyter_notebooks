{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b8224-0961-4f65-8565-13be0687bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working with Machine Learning Pipelines\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Introduction\n",
    "\n",
    "# In the previous modules we established a workflow in which we loaded some\n",
    "# data; preprocessed the data; extracted, transformed, and selected features;\n",
    "# and fit and evaluated a machine learning model.  In this module we show how\n",
    "# we can encapsulate this workflow into a [Spark MLlib\n",
    "# Pipeline](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml)\n",
    "# that we can reuse in our development process or production environment.\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ## Create a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
    "\n",
    "\n",
    "# ## Load the data\n",
    "\n",
    "# Read the enhanced ride data from HDFS:\n",
    "rides = spark.read.parquet(\"/duocar/joined_all/\")\n",
    "\n",
    "\n",
    "# ## Create the train and test sets\n",
    "\n",
    "# Create the train and test sets *before* specifying the pipeline:\n",
    "(train, test) = rides.randomSplit([0.7, 0.3], 12345)\n",
    "\n",
    "\n",
    "# ## Specify the pipeline stages\n",
    "\n",
    "# A *Pipeline* is a sequence of stages that implement a data engineering or\n",
    "# machine learning workflow.  Each stage in the pipeline is either a\n",
    "# *Transformer* or an *Estimator*.  Recall that a\n",
    "# [Transformer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer)\n",
    "# takes a DataFrame as input and returns a DataFrame as output.  Recall that an\n",
    "# [Estimator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Estimator)\n",
    "# takes a DataFrame as input and returns a Transformer (e.g., model) as output.\n",
    "# We begin by specifying the stages in our machine learning workflow.\n",
    "\n",
    "# Filter out the cancelled rides:\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "filterer = SQLTransformer(statement=\"SELECT * FROM __THIS__ WHERE cancelled == 0\")\n",
    "\n",
    "# Generate the reviewed feature:\n",
    "extractor = SQLTransformer(statement=\"SELECT *, review IS NOT NULL AS reviewed FROM __THIS__\")\n",
    "\n",
    "# Index `vehicle_color`:\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"vehicle_color\", outputCol=\"vehicle_color_indexed\")\n",
    "\n",
    "# Encode `vehicle_color_indexed`:\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"vehicle_color_indexed\"], outputCols=[\"vehicle_color_encoded\"])\n",
    "\n",
    "# Select and assemble the features:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "features = [\"reviewed\", \"vehicle_year\", \"vehicle_color_encoded\", \"CloudCover\"]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Specify the estimator (i.e., classification algorithm):\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"star_rating\", seed=23451)\n",
    "print(classifier.explainParams())\n",
    "\n",
    "# Specify the hyperparameter grid:\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
    "  .addGrid(classifier.numTrees, [20, 50, 100]) \\\n",
    "  .addGrid(classifier.subsamplingRate, [0.5, 1.0]) \\\n",
    "  .build()\n",
    "\n",
    "# Specify the evaluator:\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"star_rating\", metricName=\"accuracy\")\n",
    "\n",
    "# **Note:** We are treating `star_rating` as a multiclass label.\n",
    "\n",
    "# Specify the validator:\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "validator = TrainValidationSplit(estimator=classifier, estimatorParamMaps=paramGrid, evaluator=evaluator, seed=34512)\n",
    "\n",
    "\n",
    "# ## Specify the pipeline\n",
    "\n",
    "# A\n",
    "# [Pipeline](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline)\n",
    "# itself is an `Estimator`:\n",
    "from pyspark.ml import Pipeline\n",
    "stages = [filterer, extractor, indexer, encoder, assembler, validator]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "\n",
    "# ## Fit the pipeline model\n",
    "\n",
    "# The `fit` method produces a\n",
    "# [PipelineModel](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.PipelineModel),\n",
    "# which is a `Transformer`:\n",
    "%time pipeline_model = pipeline.fit(train)\n",
    "\n",
    "\n",
    "# ## Inspect the pipeline model\n",
    "\n",
    "# Access the stages of a `PipelineModel` instance using the `stages` attribute:\n",
    "pipeline_model.stages\n",
    "\n",
    "# ### Inspect the string indexer\n",
    "\n",
    "indexer_model = pipeline_model.stages[2]\n",
    "type(indexer_model)\n",
    "indexer_model.labels\n",
    "\n",
    "# ### Inspect the validator model\n",
    "\n",
    "validator_model = pipeline_model.stages[5]\n",
    "type(validator_model)\n",
    "validator_model.validationMetrics\n",
    "\n",
    "# ### Inspect the best random forest classifier\n",
    "\n",
    "best_model = validator_model.bestModel\n",
    "type(best_model)\n",
    "\n",
    "# Inspect the best hyperparameters:\n",
    "validator_model.bestModel._java_obj.getMaxDepth()\n",
    "validator_model.bestModel.getNumTrees\n",
    "validator_model.bestModel._java_obj.getSubsamplingRate()\n",
    "\n",
    "# **Note:** We have to access the values for `maxDepth` and `subsamplingRate`\n",
    "# from the underlying Java object.\n",
    "\n",
    "# Plot the feature importances:\n",
    "def plot_feature_importances(fi):\n",
    "  fi_array = fi.toArray()\n",
    "  plt.figure()\n",
    "  sns.barplot(list(range(len(fi_array))), fi_array)\n",
    "  plt.title(\"Feature Importances\")\n",
    "  plt.xlabel(\"Feature\")\n",
    "  plt.ylabel(\"Importance\")\n",
    "plot_feature_importances(validator_model.bestModel.featureImportances)\n",
    "\n",
    "\n",
    "# ## Save and load the pipeline model\n",
    "\n",
    "# Save the pipeline model object to our local directory in HDFS:\n",
    "pipeline_model.write().overwrite().save(\"models/pipeline_model\")\n",
    "\n",
    "# **Note**: We can use Hue to explore the saved object.\n",
    "\n",
    "# We can also use the following convenience method if we do not need to\n",
    "# overwrite an existing model:\n",
    "#```python\n",
    "#pipeline_model.save(\"models/pipeline_model\")\n",
    "#```\n",
    "\n",
    "# Load the pipeline model object from our local directory in HDFS:\n",
    "from pyspark.ml import PipelineModel\n",
    "pipeline_model_loaded = PipelineModel.read().load(\"models/pipeline_model\")\n",
    "\n",
    "# We can also use the following convenience method:\n",
    "#```python\n",
    "#pipeline_model_loaded = PipelineModel.load(\"models/pipeline_model\")\n",
    "#```\n",
    "\n",
    "# Save the underlying Java object to get around an issue with saving\n",
    "# `TrainValidationSplitModel()` objects:\n",
    "pipeline_model._to_java().write().overwrite().save(\"models/pipeline_model\")\n",
    "\n",
    "\n",
    "# ## Apply the pipeline model\n",
    "\n",
    "# Use the `transform` method to apply the `PipelineModel` to the test set:\n",
    "classified = pipeline_model_loaded.transform(test)\n",
    "classified.printSchema()\n",
    "\n",
    "\n",
    "# ## Evaluate the pipeline model\n",
    "\n",
    "# Generate a confusion matrix:\n",
    "classified \\\n",
    "  .groupBy(\"prediction\") \\\n",
    "  .pivot(\"star_rating\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"prediction\") \\\n",
    "  .fillna(0) \\\n",
    "  .show()\n",
    "\n",
    "# Evaluate the random forest model:\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"star_rating\", metricName=\"accuracy\")\n",
    "evaluator.evaluate(classified)\n",
    "\n",
    "# Compare to the baseline prediction (always predict five-star rating):\n",
    "from pyspark.sql.functions import lit\n",
    "classified_with_baseline = classified.withColumn(\"prediction_baseline\", lit(5.0))\n",
    "evaluator.setPredictionCol(\"prediction_baseline\").evaluate(classified_with_baseline)\n",
    "\n",
    "# Our random forest classifier is doing no better than always predicting a\n",
    "# five-star rating.  We can try to improve our model by adding more features,\n",
    "# experimenting with additional hyperparameter combinations, and exploring\n",
    "# other machine learning algorithms.\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Import the\n",
    "# [RFormula](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula)\n",
    "# class from the `pyspark.ml.feature` module.\n",
    "\n",
    "# (2) Create an instance of the `RFormula` class with the R formula\n",
    "# `star_rating ~ reviewed + vehicle_year + vehicle_color`.\n",
    "\n",
    "# (3) Specify a pipeline consisting of the `filterer`, `extractor`, and the\n",
    "# RFormula instance specified above.\n",
    "\n",
    "# (4) Fit the pipeline on the `train` DataFrame.\n",
    "\n",
    "# (5) Use the `save` method to save the pipeline model to the\n",
    "# `models/my_pipeline_model` directory in HDFS.\n",
    "\n",
    "# (6) Import the `PipelineModel` class from the `pyspark.ml` package.\n",
    "\n",
    "# (7) Use the `load` method of the `PipelineModel` class to load the saved\n",
    "# pipeline model.\n",
    "\n",
    "# (8) Apply the loaded pipeline model to the test set and examine the resulting\n",
    "# DataFrame.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Spark Documentation - ML Pipelines](http://spark.apache.org/docs/latest/ml-pipeline.html)\n",
    "\n",
    "# [Spark Python API - pyspark.ml package](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html)\n",
    "\n",
    "# [Spark Python API - MLReader class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.util.MLReader)\n",
    "\n",
    "# [Spark Python API - MLWriter class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.util.MLWriter)\n",
    "\n",
    "\n",
    "# ## Stop the SparkSession\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
