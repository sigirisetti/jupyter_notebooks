{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e3a9f-ace6-4b31-bd2c-36185ba66ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # User-Defined Functions\n",
    "\n",
    "# Copyright © 2010–2020 Cloudera. All rights reserved.\n",
    "# Not to be reproduced or shared without prior written \n",
    "# consent from Cloudera.\n",
    "\n",
    "\n",
    "# ## Overview\n",
    "\n",
    "# In this module we demonstrate how to create and apply user-defined functions.\n",
    "\n",
    "\n",
    "# ## User-Defined Functions\n",
    "\n",
    "# * It is relatively easy to create and apply a user-defined function (UDF)\n",
    "#   * Define a Python function that operates on a row of data\n",
    "#   * Register the Python function as a UDF and specify the return type\n",
    "#   * Apply the UDF as if it were a built-in function\n",
    "\n",
    "# * Python and any required packages must be installed on the worker nodes\n",
    "#   * It is possible to distribute required packages via Spark\n",
    "\n",
    "# * Built-in functions are more efficient than user-defined functions\n",
    "#   * Use built-in functions when available\n",
    "#   * Create user-defined functions only when necessary\n",
    "\n",
    "# * User-defined function are inefficient because of the following:\n",
    "#   * A Python process must be started alongside each executor\n",
    "#   * Data must be converted between Java and Python types\n",
    "#   * Data must be transferred between the Java and Python processes\n",
    "\n",
    "# * To improve the performance of a UDF:\n",
    "#   * Use the [Apache Arrow](https://arrow.apache.org/) platform \n",
    "#   * Use a vectorized UDF (see the `pandas_udf` function)\n",
    "#   * Rewrite the UDF in Scala or Java\n",
    "\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"udfs\").getOrCreate()\n",
    "\n",
    "# Read the raw ride data from HDFS:\n",
    "rides = spark.read.csv(\"/duocar/raw/rides/\", header=True, inferSchema=True)\n",
    "\n",
    "# Cast `date_time` to a timestamp:\n",
    "from pyspark.sql.functions import col\n",
    "rides_clean = rides.withColumn(\"date_time\", col(\"date_time\").cast(\"timestamp\"))\n",
    "\n",
    "\n",
    "# ## Example 1: Hour of Day\n",
    "\n",
    "# Define the Python function:\n",
    "import datetime\n",
    "def hour_of_day(timestamp):\n",
    "  return timestamp.hour\n",
    "\n",
    "# **Note:** The Spark `TimestampType` corresponds to Python `datetime.datetime`\n",
    "# objects.\n",
    "\n",
    "# Test the Python function:\n",
    "dt = datetime.datetime(2017, 7, 21, 5, 51, 10)\n",
    "hour_of_day(dt)\n",
    "\n",
    "# Register the Python function as a UDF:\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "hour_of_day_udf = udf(hour_of_day, returnType=IntegerType())\n",
    "\n",
    "# **Note:** We must explicitly specify the return type otherwise it defaults\n",
    "# to `StringType`.\n",
    "\n",
    "# Apply the UDF:\n",
    "rides_clean \\\n",
    "  .select(\"date_time\", hour_of_day_udf(\"date_time\")) \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "# Use the UDF to compute the number of rides by hour of day:\n",
    "rides_clean \\\n",
    "  .select(hour_of_day_udf(\"date_time\").alias(\"hour_of_day\")) \\\n",
    "  .groupBy(\"hour_of_day\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"hour_of_day\") \\\n",
    "  .show(25)\n",
    "\n",
    "\n",
    "# ## Example 2: Great-Circle Distance\n",
    "\n",
    "# The [great-circle\n",
    "# distance](https://en.wikipedia.org/wiki/Great-circle_distance) is the\n",
    "# shortest distance between two points on the surface of a sphere.  In this\n",
    "# example we create a user-defined function to compute the [haversine\n",
    "# approximation](https://en.wikipedia.org/wiki/Haversine_formula) to\n",
    "# the great-circle distance between the ride origin and destination.\n",
    "\n",
    "# Define the haversine function (based on the code at\n",
    "# [rosettacode.org](http://rosettacode.org/wiki/Haversine_formula#Python)):\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "  \"\"\"\n",
    "  Return the haversine approximation to the great-circle distance between two\n",
    "  points (in meters).\n",
    "  \"\"\"\n",
    "  R = 6372.8 # Earth radius in kilometers\n",
    " \n",
    "  dLat = radians(lat2 - lat1)\n",
    "  dLon = radians(lon2 - lon1)\n",
    "\n",
    "  lat1 = radians(lat1)\n",
    "  lat2 = radians(lat2)\n",
    " \n",
    "  a = sin(dLat / 2.0)**2 + cos(lat1) * cos(lat2) * sin(dLon / 2.0)**2\n",
    "  c = 2.0 * asin(sqrt(a))\n",
    " \n",
    "  return R * c * 1000.0\n",
    "\n",
    "# **Note:** We have made some minor changes to the code to make it integer\n",
    "# proof.\n",
    "\n",
    "# Test the Python function:\n",
    "haversine(36.12, -86.67, 33.94, -118.40)  # = 2887259.9506071107:\n",
    "\n",
    "# Register the Python function as a UDF:\n",
    "from pyspark.sql.types import DoubleType\n",
    "haversine_udf = udf(haversine, returnType=DoubleType())\n",
    "\n",
    "# Apply the haversine UDF:\n",
    "distances = rides \\\n",
    "  .withColumn(\"haversine_approximation\", haversine_udf(\"origin_lat\", \"origin_lon\", \"dest_lat\", \"dest_lon\")) \\\n",
    "  .select(\"distance\", \"haversine_approximation\")\n",
    "distances.show(5)\n",
    "\n",
    "# We expect the haversine approximation to be less than the ride distance:\n",
    "distances \\\n",
    "  .select((col(\"haversine_approximation\") > col(\"distance\")).alias(\"haversine > distance\")) \\\n",
    "  .groupBy(\"haversine > distance\") \\\n",
    "  .count() \\\n",
    "  .show()\n",
    "\n",
    "# The null values correspond to cancelled rides:\n",
    "rides.filter(col(\"cancelled\") == 1).count()\n",
    "\n",
    "# The true values reflect the fact that the haversine formula is only an\n",
    "# approximation to the great-circle distance:\n",
    "distances.filter(col(\"haversine_approximation\") > col(\"distance\")).show(5)\n",
    "\n",
    "\n",
    "# ## Exercises\n",
    "\n",
    "# (1) Create a UDF that extracts the day of the week from a timestamp column.\n",
    "# **Hint:** Use the\n",
    "# [weekday](https://docs.python.org/2/library/datetime.html#datetime.datetime.weekday)\n",
    "# method of the Python `datetime` class.  \n",
    "\n",
    "def day_of_week(timestamp):\n",
    "  return timestamp.today().weekday()\n",
    "\n",
    "day_of_week_udf = udf(day_of_week, returnType=IntegerType())\n",
    "\n",
    "# **Note:** We must explicitly specify the return type otherwise it defaults\n",
    "# to `StringType`.\n",
    "\n",
    "rides_clean.printSchema()\n",
    "\n",
    "# Apply the UDF:\n",
    "rides_clean \\\n",
    "  .select(\"date_time\", day_of_week_udf(\"date_time\")) \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "# Use the UDF to compute the number of rides by hour of day:\n",
    "rides_clean \\\n",
    "  .select(day_of_week_udf(\"date_time\").alias(\"day_of_week\")) \\\n",
    "  .groupBy(\"day_of_week\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"day_of_week\") \\\n",
    "  .show(25)\n",
    "\n",
    "\n",
    "# (2) Use the UDF to compute the number of rides by day of week.\n",
    "\n",
    "# (3) Use the built-in function `dayofweek` to compute the number of rides by day of week.\n",
    "\n",
    "\n",
    "# ## References\n",
    "\n",
    "# [Python API - datetime\n",
    "# module](https://docs.python.org/2/library/datetime.html)\n",
    "\n",
    "# [Spark Python API -\n",
    "# pyspark.sql.functions.udf](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf)\n",
    "\n",
    "# [Spark Python API -\n",
    "# pyspark.sql.functions.pandas_udf](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf)\n",
    "\n",
    "# [Cloudera Engineering Blog - Working with UDFs in Apache\n",
    "# Spark](https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/)\n",
    "\n",
    "# [Cloudera Engineering Blog - Use your favorite Python library on PySpark\n",
    "# cluster with Cloudera Data Science\n",
    "# Workbench](https://blog.cloudera.com/blog/2017/04/use-your-favorite-python-library-on-pyspark-cluster-with-cloudera-data-science-workbench/)\n",
    "\n",
    "\n",
    "# ## Cleanup\n",
    "\n",
    "# Stop the SparkSession:\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
